{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1dGzrSs2EycY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau , StepLR\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import vit_b_16\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import os\n",
        "import yaml\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, lfilter, iirnotch, filtfilt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter, iirnotch, filtfilt\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset\n",
        "from skimage.transform import resize\n",
        "import torch\n",
        "import random\n",
        "#from utils.data_utils import createTrainTestSplit,create_k_fold_splits,analyze_checkpoints\n",
        "#from utils.data_utils import plot_spectrograms\n",
        "#from utils.training_utils import parallel_grid_search \n",
        "#from data.dataset import HMS_EEG_Dataset\n",
        "#from data.dataset import HMS_Spectrogram_Dataset\n",
        "#from data.dataset import CombinedDataset\n",
        "#from models.models import  EEGNet, DeepConvNet\n",
        "#from training.training import train_and_validate_eeg_manual_lr_grid_search, train_and_validate_eeg, train_and_validate_from_checkpoint\n",
        "#from utils.config_loader import load_config\n",
        "#from utils.logger_utils import setup_logger\n",
        "from contextlib import redirect_stdout\n",
        "from itertools import product\n",
        "import io\n",
        "import sys\n",
        "from io import StringIO\n",
        "from itertools import product\n",
        "import numpy as np\n",
        "import torch.multiprocessing as mp\n",
        "import torch.distributed as dist\n",
        "import torch\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_entirely_nan(eeg_id):\n",
        "    eeg_data = load_train_eeg_frame(eeg_id)\n",
        "    return np.isnan(eeg_data.values).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_train_spectr_frame(id):\n",
        "    # Ensure the ID is an integer to prevent file path errors\n",
        "    id = int(id)\n",
        "    # Construct the file path using the integer ID\n",
        "    file_path = os.path.join(TRAIN_SPECTR, f'{id}.parquet')\n",
        "    # Load the spectrogram data from the specified Parquet file\n",
        "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
        "    # Optional: Verify that the columns match expected Spectrogram columns\n",
        "    if not CFG['SKIP_ASSERT']:\n",
        "        assert list(data.columns) == CFG['SPECTR_COLUMNS'], 'Spectrogram columns order is not the same!'\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration from the YAML file\n",
        "config_file = \"/eng/home/koushani/Documents/Multimodal_XAI/Brain-Pattern-Identification-using-Multimodal-Classification/root/config/config.yml\"\n",
        "with open(config_file, 'r') as file:\n",
        "    CFG = yaml.safe_load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded metadata from /data2/users/koushani/HMS_data\n"
          ]
        }
      ],
      "source": [
        "metadata = pd.read_csv('/data2/users/koushani/HMS_data/train.csv')\n",
        "print(f\"Loaded metadata from {CFG['root_dir']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to check if the EEG data is entirely NaN\n",
        "def is_entirely_nan(eeg_id):\n",
        "    eeg_data = load_train_eeg_frame(eeg_id)\n",
        "    return np.isnan(eeg_data.values).all()\n",
        "\n",
        "def mirror_eeg(data):\n",
        "    # Extract the relevant lists from the config\n",
        "    LL = CFG['RL']  # The 'RL' key is actually referencing the 'LL' list\n",
        "    LP = CFG['LP']\n",
        "    RL = CFG['RP']  # The 'RP' key is referencing the 'LP' list\n",
        "    RP = CFG['RL']  # The 'RL' key is also referencing the 'LL' list\n",
        "\n",
        "    # Assuming feature_to_index is a dictionary that maps feature names to indices\n",
        "    indx1 = [feature_to_index[x] for x in LL + LP if x in feature_to_index]\n",
        "    indx2 = [feature_to_index[x] for x in RL + RP if x in feature_to_index]\n",
        "\n",
        "    # Swap the data using the indices\n",
        "    data[indx1, :], data[indx2, :] = data[indx2, :], data[indx1, :]\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def load_train_eeg_frame(id):\n",
        "    # Ensure the ID is an integer to avoid issues with file name construction\n",
        "    id = int(id)\n",
        "    # Construct the file path using the integer ID\n",
        "    file_path = os.path.join(TRAIN_EEGS, f'{id}.parquet')\n",
        "    # Load the EEG data from the specified Parquet file\n",
        "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
        "    # Optional: Verify that the columns match expected EEG columns\n",
        "    if not CFG['SKIP_ASSERT']:\n",
        "        assert list(data.columns) == CFG['EEG_COLUMNS'], 'EEG columns order is not the same!'\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_train_spectr_frame(id):\n",
        "    # Ensure the ID is an integer to prevent file path errors\n",
        "    id = int(id)\n",
        "    # Construct the file path using the integer ID\n",
        "    file_path = os.path.join(TRAIN_SPECTR, f'{id}.parquet')\n",
        "    # Load the spectrogram data from the specified Parquet file\n",
        "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
        "    # Optional: Verify that the columns match expected Spectrogram columns\n",
        "    if not CFG['SKIP_ASSERT']:\n",
        "        assert list(data.columns) == CFG['SPECTR_COLUMNS'], 'Spectrogram columns order is not the same!'\n",
        "    return data\n",
        "\n",
        "\n",
        "# Plot raw and processed spectrograms\n",
        "def plot_spectrograms(raw, processed, labels, num_labels):\n",
        "    \"\"\"Plot raw and processed spectrograms.\"\"\"\n",
        "    x_ticks = np.linspace(0, processed.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = [labels[i] for i in x_ticks]\n",
        "\n",
        "    plt.figure(figsize=(40, 16))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Raw Signal\")\n",
        "    plt.imshow(raw, aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Processed Signal\")\n",
        "    if processed.ndim == 3 and processed.shape[2] > 1:\n",
        "        plt.imshow(processed[:, :, 0], aspect='auto', cmap='viridis')\n",
        "    else:\n",
        "        plt.imshow(processed.squeeze(), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def baseline_correction(sig):\n",
        "    sig -= np.mean(sig, axis=0)\n",
        "    return sig\n",
        "\n",
        "def normalize_signal(sig):\n",
        "    \"\"\"Normalize the signal by scaling it to the range [0, 1], handling NaN values.\"\"\"\n",
        "    sig = np.nan_to_num(sig, nan=np.nanmean(sig))\n",
        "    return (sig - np.min(sig)) / (np.max(sig) - np.min(sig) + 1e-6)\n",
        "\n",
        "def apply_notch_filter(sig, freq=60, fs=200, quality=30):\n",
        "    b, a = iirnotch(freq, quality, fs)\n",
        "    sig = filtfilt(b, a, sig, axis=0)\n",
        "    return sig\n",
        "\n",
        "def smooth_spectrogram(sig, sigma=1.0):\n",
        "    sig = gaussian_filter(sig, sigma=sigma)\n",
        "    return sig\n",
        "\n",
        "def resample_spectrogram(sig, target_shape):\n",
        "    sig = resize(sig, target_shape, mode='reflect', anti_aliasing=True)\n",
        "    return sig\n",
        "\n",
        "def handle_nan(data):\n",
        "    data = data[~np.isnan(data).all(axis=1)]\n",
        "    if data.size == 0:\n",
        "        data = np.zeros_like(data)\n",
        "    else:\n",
        "        where_nan = np.isnan(data)\n",
        "        mean_values = np.nanmean(data, axis=1, keepdims=True)\n",
        "        mean_values[np.isnan(mean_values)] = 0\n",
        "        data[where_nan] = np.take(mean_values, np.where(where_nan)[0])\n",
        "    return data\n",
        "\n",
        "def pad_or_truncate(data, length):\n",
        "    if isinstance(length, int):\n",
        "        if data.shape[1] < length:\n",
        "            padding = np.zeros((data.shape[0], length - data.shape[1]))\n",
        "            data = np.hstack((data, padding))\n",
        "        else:\n",
        "            data = data[:, :length]\n",
        "    elif isinstance(length, tuple):\n",
        "        target_rows, target_cols = length\n",
        "        if data.shape[0] < target_rows:\n",
        "            row_padding = np.zeros((target_rows - data.shape[0], data.shape[1]))\n",
        "            data = np.vstack((data, row_padding))\n",
        "        else:\n",
        "            data = data[:target_rows, :]\n",
        "        if data.shape[1] < target_cols:\n",
        "            col_padding = np.zeros((data.shape[0], target_cols - data.shape[1]))\n",
        "            data = np.hstack((data, col_padding))\n",
        "        else:\n",
        "            data = data[:, :target_cols]\n",
        "    return data\n",
        "\n",
        "def calculate_differential_signals(data):\n",
        "    \"\"\"\n",
        "    Calculate differential signals based on pairs of features and concatenate them to the original data.\n",
        "    \n",
        "    Parameters:\n",
        "    - data: numpy.ndarray, shape (n_channels, n_samples)\n",
        "        The EEG data to process.\n",
        "    - map_features: list of tuples\n",
        "        Each tuple contains a pair of feature names (feat_a, feat_b) where the differential signal is calculated as data[feat_a] - data[feat_b].\n",
        "    - feature_to_index: dict\n",
        "        A dictionary mapping feature names to their respective indices in `data`.\n",
        "    \n",
        "    Returns:\n",
        "    - combined_data: numpy.ndarray\n",
        "        The original data concatenated with the calculated differential signals.\n",
        "    \"\"\"\n",
        "    num_pairs = len(CFG['map_features'])\n",
        "    differential_data = np.zeros((num_pairs, data.shape[1]))\n",
        "    \n",
        "    for i, (feat_a, feat_b) in enumerate(CFG['map_features']):\n",
        "        if feat_a in feature_to_index and feat_b in feature_to_index:\n",
        "            differential_data[i, :] = data[feature_to_index[feat_a], :] - data[feature_to_index[feat_b], :]\n",
        "        else:\n",
        "            print(f\"Skipping: Feature {feat_a} or {feat_b} not found in feature_to_index\")\n",
        "            differential_data[i, :] = np.zeros(data.shape[1])\n",
        "\n",
        "    # Debugging: Print the shapes before concatenation\n",
        "    #print(f\"Shape of data: {data.shape}\")\n",
        "    #print(f\"Shape of differential_data: {differential_data.shape}\")\n",
        "\n",
        "    # Ensure that the shapes match before concatenation \n",
        "    combined_data = np.vstack((data, differential_data))\n",
        "    return combined_data\n",
        "    \n",
        "\n",
        "def butter_bandpass(CFG):\n",
        "    nyquist = 0.5 * CFG['sampling_rate']\n",
        "    low = CFG['bandpass_filter']['low'] / nyquist\n",
        "    high = CFG['bandpass_filter']['high'] / nyquist\n",
        "    return butter(CFG['bandpass_filter']['order'], [low, high], btype='band')\n",
        "\n",
        "def butter_bandpass_filter(data, CFG):\n",
        "    b, a = butter_bandpass(CFG)\n",
        "    return lfilter(b, a, data, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "def normalize(data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        return (data - mean) / (std + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "\n",
        "def denoise_filter(x, CFG):\n",
        "    y = butter_bandpass_filter(x, CFG)\n",
        "    y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
        "    y = y[:, 0:-1:4]\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "# data = select_and_map_channels(data, self.cfg['eeg_features'], self.feature_to_index, self.differential_channels_start_index)\n",
        "def select_and_map_channels(data, channels, differential_channels_start_index):\n",
        "    selected_indices = [feature_to_index[ch] for ch in channels if ch in feature_to_index]\n",
        "    differential_indices = list(range(differential_channels_start_index, differential_channels_start_index + len(CFG['map_features'])))\n",
        "    selected_data = data[selected_indices + differential_indices, :]\n",
        "    return selected_data\n",
        "\n",
        "def labels_to_probabilities(labels, num_classes):\n",
        "    labels = torch.eye(num_classes)[labels]\n",
        "    return labels\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer, new_checkpoint=False):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    \n",
        "    if not new_checkpoint and os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        valid_losses = checkpoint['valid_losses']\n",
        "        train_accuracies = checkpoint['train_accuracies']\n",
        "        valid_accuracies = checkpoint['valid_accuracies']\n",
        "        lr_scheduler = checkpoint['lr_scheduler']\n",
        "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
        "    else:\n",
        "        if new_checkpoint:\n",
        "            print(f\"Creating a new checkpoint at '{checkpoint_path}'\")\n",
        "        else:\n",
        "            print(f\"No checkpoint found at '{checkpoint_path}', starting fresh.\")\n",
        "        start_epoch = 0\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        train_accuracies = []\n",
        "        valid_accuracies = []\n",
        "        lr_scheduler = []\n",
        "\n",
        "    return start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies, lr_scheduler\n",
        "\n",
        "def save_checkpoint(state, checkpoint_dir, checkpoint_filename):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at '{checkpoint_path}'\")\n",
        "\n",
        "\n",
        "def detect_and_save_checkpoint(state, checkpoint_dir, optimizer, regularization_lambda):\n",
        "    # Detect changes in optimizer and regularization parameter\n",
        "    optimizer_changed = CFG['last_optimizer'] is None or type(optimizer) != CFG['last_optimizer']\n",
        "    regularization_changed = CFG.last_regularization_lambda is None or regularization_lambda != CFG['last_regularization_lambda']\n",
        "\n",
        "    # Initialize the checkpoint filename\n",
        "    checkpoint_filename = \"checkpoint.pth.tar\"\n",
        "\n",
        "    # Modify the checkpoint filename based on the changes detected\n",
        "    if optimizer_changed and regularization_changed:\n",
        "        checkpoint_filename = \"checkpoint_optimizer_and_regularization.pth.tar\"\n",
        "    elif optimizer_changed:\n",
        "        checkpoint_filename = \"checkpoint_optimizer.pth.tar\"\n",
        "    elif regularization_changed:\n",
        "        checkpoint_filename = \"checkpoint_regularization.pth.tar\"\n",
        "\n",
        "    if optimizer_changed or regularization_changed:\n",
        "        print(f\"Changes detected in {'optimizer' if optimizer_changed else ''} {'and' if optimizer_changed and regularization_changed else ''} {'regularization parameter' if regularization_changed else ''}. Creating a new checkpoint.\")\n",
        "        CFG['last_optimizer'] = type(optimizer)\n",
        "        CFG['last_regularization_lambda'] = regularization_lambda\n",
        "        save_checkpoint(state, checkpoint_dir, checkpoint_filename)\n",
        "        \n",
        "        \n",
        "def create_k_fold_splits(metadata, n_splits=5):\n",
        "    # Drop unnecessary columns\n",
        "    metadata.drop(columns=[\n",
        "            'eeg_sub_id',\n",
        "            'spectrogram_sub_id',\n",
        "            'patient_id',\n",
        "            'label_id'\n",
        "        ], inplace=True)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    metadata['eeg_id'] = metadata['eeg_id'].astype(int)\n",
        "    metadata['spectrogram_id'] = metadata['spectrogram_id'].astype(int)\n",
        "    metadata['eeg_label_offset_seconds'] = metadata['eeg_label_offset_seconds'].astype(int)\n",
        "    metadata['spectrogram_label_offset_seconds'] = metadata['spectrogram_label_offset_seconds'].astype(int)\n",
        "\n",
        "    # Debugging: Sample the data if in DEBUG mode to reduce size for faster processing\n",
        "    if CFG['debug']:\n",
        "        metadata = metadata.sample(min(CFG['debug_input_size'], len(metadata)))\n",
        "\n",
        "    # Extract features and labels for stratification\n",
        "    X = metadata['eeg_id']\n",
        "    y = metadata['expert_consensus']  # Correct column name for class labels\n",
        "\n",
        "    # Create stratified K-Folds\n",
        "    skf = StratifiedKFold(n_splits=n_splits)\n",
        "    fold_indices = []\n",
        "\n",
        "    for train_index, valid_index in skf.split(X, y):\n",
        "        train_ids = X.iloc[train_index].tolist()\n",
        "        valid_ids = X.iloc[valid_index].tolist()\n",
        "        fold_indices.append((train_ids, valid_ids))\n",
        "\n",
        "    return fold_indices\n",
        "\n",
        "def createTrainTestSplit(metadata, fold_indices, fold_idx):\n",
        "    train_ids, valid_ids = fold_indices[fold_idx]\n",
        "\n",
        "    train_metadata = metadata[metadata['eeg_id'].isin(train_ids)]\n",
        "    valid_metadata = metadata[metadata['eeg_id'].isin(valid_ids)]\n",
        "\n",
        "    return train_metadata, valid_metadata\n",
        "\n",
        "\n",
        "\n",
        "def seed_everything():\n",
        "    np.random.seed(CFG['seed'])\n",
        "    torch.manual_seed(CFG['seed'])\n",
        "    random.seed(CFG['seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(CFG['seed'])\n",
        "        torch.cuda.manual_seed_all(CFG['seed'])\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "    # Get the current default CUDA device\n",
        "        device = torch.device(\"cuda\")\n",
        "    # Get the name of the device\n",
        "        device_name = torch.cuda.get_device_name(device)\n",
        "        print(f\"CUDA is available. Using device: {device} ({device_name})\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"CUDA is not available. Using CPU.\")\n",
        "    return device\n",
        "\n",
        "\n",
        "def stop_checkpointing():\n",
        "    CFG['checkpointing_enabled'] = False\n",
        "    print(\"Checkpointing disabled.\")\n",
        "\n",
        "def start_checkpointing():\n",
        "    CFG['checkpointing_enabled'] = True\n",
        "    print(\"Checkpointing enabled.\")\n",
        "\n",
        "\n",
        "def calculate_metrics(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Calculate precision, recall, and F1 scores for a given model and dataloader.\n",
        "    \n",
        "    Args:\n",
        "    - model (torch.nn.Module): Trained model.\n",
        "    - dataloader (torch.utils.data.DataLoader): DataLoader for the validation/test data.\n",
        "    - device (torch.device): Device to perform calculations on (CPU/GPU).\n",
        "    \n",
        "    Returns:\n",
        "    - precision (float): Precision score.\n",
        "    - recall (float): Recall score.\n",
        "    - f1 (float): F1 score.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, labels in dataloader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            _, labels_max = torch.max(labels, 1)\n",
        "            \n",
        "            all_labels.extend(labels_max.cpu().numpy())\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "    \n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    \n",
        "    return precision, recall, f1\n",
        "\n",
        "def plot_metrics(train_metrics, valid_metrics, metric_name, save_dir=CFG['save_dir']):\n",
        "    \"\"\"\n",
        "    Plot and save the training and validation metrics over epochs.\n",
        "    \n",
        "    Args:\n",
        "    - train_metrics (list): List of training metrics (precision, recall, or F1).\n",
        "    - valid_metrics (list): List of validation metrics (precision, recall, or F1).\n",
        "    - metric_name (str): Name of the metric being plotted.\n",
        "    - save_dir (str): Directory to save the generated plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_metrics, label=f'Train {metric_name}')\n",
        "    plt.plot(valid_metrics, label=f'Validation {metric_name}')\n",
        "    plt.title(f'{metric_name} Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    \n",
        "    # Ensure the save directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    # Save the plot with the specified metric name\n",
        "    plot_path = os.path.join(save_dir, f'EEG_MODEL_{metric_name}.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"{metric_name} plot saved at {plot_path}\")\n",
        "\n",
        "\n",
        "def plot_learning_rate_and_regularization(lr_scheduler, regularization_losses, save_dir=CFG['save_dir']):\n",
        "    \"\"\"\n",
        "    Plot and save the learning rate schedule and regularization loss over epochs.\n",
        "\n",
        "    Args:\n",
        "    - lr_scheduler (list): List of learning rates over epochs.\n",
        "    - regularization_losses (list): List of regularization losses over epochs.\n",
        "    - save_dir (str): Directory to save the generated plot.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(lr_scheduler)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(regularization_losses)\n",
        "    plt.title('Regularization Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Regularization Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Ensure the save directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save the plot with the specified name\n",
        "    plot_path = os.path.join(save_dir, 'EEG_MODEL_LearningRate_and_Regularization.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Learning Rate and Regularization Loss plot saved at {plot_path}\")\n",
        "\n",
        "def plot_accuracies(train_accuracies, valid_accuracies, save_dir=CFG['save_dir']):\n",
        "    \"\"\"\n",
        "    Plot and save the training and validation accuracies over epochs.\n",
        "\n",
        "    Args:\n",
        "    - train_accuracies (list): List of training accuracies over epochs.\n",
        "    - valid_accuracies (list): List of validation accuracies over epochs.\n",
        "    - save_dir (str): Directory to save the generated plot.\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(train_accuracies) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    \n",
        "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, valid_accuracies, 'r-', label='Validation Accuracy')\n",
        "    \n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Ensure the save directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save the plot with the specified name\n",
        "    plot_path = os.path.join(save_dir, 'EEG_MODEL_Accuracy.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Training and Validation Accuracy plot saved at {plot_path}\")\n",
        "    \n",
        "    \n",
        "    \n",
        "def plot_confusion_matrix(y_true, y_pred, classes, save_dir=CFG['save_dir'], normalize=False):\n",
        "    \"\"\"\n",
        "    Plot and save the confusion matrix.\n",
        "\n",
        "    Args:\n",
        "    - y_true (list): True labels.\n",
        "    - y_pred (list): Predicted labels.\n",
        "    - classes (list): List of class names.\n",
        "    - save_dir (str): Directory to save the generated plot.\n",
        "    - normalize (bool): If True, normalize the confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\".2f\" if normalize else \"d\", cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Ensure the save directory exists\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Save the plot with the specified name\n",
        "    plot_path = os.path.join(save_dir, 'Confusion_Matrix.png')\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Confusion matrix plot saved at {plot_path}\")\n",
        "\n",
        "def create_confusion_matrix(model, dataloader, classes, device=CFG['device'], checkpoint_dir=CFG['checkpoint_dir'], checkpoint_filename='eeg_checkpoint.pth.tar', save_dir=CFG['save_dir']):\n",
        "    \"\"\"\n",
        "    Load checkpoint, make predictions on the validation set, and plot the confusion matrix.\n",
        "\n",
        "    Args:\n",
        "    - model (torch.nn.Module): The model architecture.\n",
        "    - dataloader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
        "    - classes (list): List of class names.\n",
        "    - device (str): Device to perform computation on ('cuda' or 'cpu').\n",
        "    - checkpoint_dir (str): Directory containing the checkpoint.\n",
        "    - checkpoint_filename (str): Filename of the checkpoint.\n",
        "    - save_dir (str): Directory to save the generated plot.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(model.parameters())  # Dummy optimizer to load state dict\n",
        "    start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies, lr_scheduler, regularization_losses = load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True).squeeze()  # Get the index of the max log-probability\n",
        "            y_true.extend(target.cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "\n",
        "    plot_confusion_matrix(y_true, y_pred, classes, save_dir=save_dir)\n",
        "    \n",
        "def load_checkpoint_for_analysis(checkpoint_path):\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        valid_accuracies = checkpoint['valid_accuracies']  # Extract validation accuracies\n",
        "        return valid_accuracies\n",
        "    else:\n",
        "        print(f\"No checkpoint found at {checkpoint_path}\")\n",
        "        return None\n",
        "\n",
        "def analyze_checkpoints(checkpoint_dir):\n",
        "\n",
        "    best_valid_acc = 0\n",
        "    best_gamma = None\n",
        "    best_step_size = None\n",
        "\n",
        "    # Loop through all model files in the directory\n",
        "    for filename in os.listdir(checkpoint_dir):\n",
        "        if filename.startswith(\"model_\"):\n",
        "            # Extract gamma and step_size from filename (assumes filenames contain gamma and decay)\n",
        "            parts = filename.split(\"_\")\n",
        "            gamma = float(parts[2])\n",
        "            step_size = int(parts[4].split(\".\")[0])  # Extract the number before .pth\n",
        "        \n",
        "            # Load the checkpoint\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
        "            valid_accuracies = load_checkpoint_for_analysis(checkpoint_path)\n",
        "\n",
        "        if valid_accuracies:\n",
        "                # Get the maximum validation accuracy from this run\n",
        "            max_valid_acc = max(valid_accuracies)\n",
        "            print(f\"Model: {filename} | Max Validation Accuracy: {max_valid_acc}\")\n",
        "\n",
        "                # Compare with the best validation accuracy\n",
        "            if max_valid_acc > best_valid_acc:\n",
        "                best_valid_acc = max_valid_acc\n",
        "                best_gamma = gamma\n",
        "                best_step_size = step_size\n",
        "\n",
        "    print(f\"Best model found with gamma={best_gamma}, step_size={best_step_size}, validation accuracy={best_valid_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_to_index = {x: y for x, y in zip(CFG['eeg_features'], range(len(CFG['eeg_features'])))}\n",
        "ROOT_DIR = '/data2/users/koushani/HMS_data'\n",
        "TRAIN_EEGS = os.path.join(ROOT_DIR , 'train_eegs')\n",
        "TRAIN_SPECTR =  os.path.join(ROOT_DIR, 'train_spectrograms')\n",
        "TEST_EEGS = os.path.join(ROOT_DIR, 'test_eegs')\n",
        "TEST_SPECTR = os.path.join(ROOT_DIR, 'test_spectrograms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded metadata from /data2/users/koushani/HMS_data\n"
          ]
        }
      ],
      "source": [
        "metadata = pd.read_csv('/data2/users/koushani/HMS_data/train.csv')\n",
        "print(f\"Loaded metadata from {CFG['root_dir']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created K-Fold splits.\n",
            "Processing fold 1/5\n",
            "Processing fold 2/5\n",
            "Processing fold 3/5\n",
            "Processing fold 4/5\n",
            "Processing fold 5/5\n"
          ]
        }
      ],
      "source": [
        "fold_indices = create_k_fold_splits(metadata, n_splits=5)\n",
        "print(\"Created K-Fold splits.\")\n",
        "    \n",
        "\n",
        "for fold_idx in range(len(fold_indices)):\n",
        "    train_metadata, valid_metadata = createTrainTestSplit(metadata, fold_indices, fold_idx)\n",
        "    \n",
        "    print(f\"Processing fold {fold_idx + 1}/{len(fold_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HMS_Spectrogram_Dataset class\n",
        "class HMS_Spectrogram_Dataset(Dataset):\n",
        "    def __init__(self, train_ids, cfg, augmentations=None, plot=False):\n",
        "        super(HMS_Spectrogram_Dataset, self).__init__()\n",
        "        self.train_ids = train_ids\n",
        "        self.cfg = cfg\n",
        "        self.augmentations = augmentations\n",
        "        self.plot = plot  # Flag to control plotting\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.train_ids.iloc[idx]\n",
        "        spec_id = row['spectrogram_id']\n",
        "        raw_spectrogram = load_train_spectr_frame(spec_id)\n",
        "        label_name = row['expert_consensus']\n",
        "        label_idx = self.cfg['name2label'][label_name]\n",
        "        label = labels_to_probabilities(label_idx, self.cfg['n_classes'])\n",
        "        offset = row.get(\"spectrogram_label_offset_seconds\", None)\n",
        "\n",
        "        if isinstance(raw_spectrogram, pd.DataFrame):\n",
        "            raw_spectrogram = raw_spectrogram.to_numpy()\n",
        "\n",
        "        if offset is not None:\n",
        "            offset = offset // 2\n",
        "            basic_spectrogram = raw_spectrogram[:, offset:offset + 300]\n",
        "            pad_size = max(0, 300 - basic_spectrogram.shape[1])\n",
        "            basic_spectrogram = np.pad(basic_spectrogram, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        else:\n",
        "            basic_spectrogram = raw_spectrogram\n",
        "\n",
        "        spectrogram = basic_spectrogram.T\n",
        "        processed_spectrogram = pad_or_truncate(spectrogram, self.cfg['image_size'])\n",
        "        processed_spectrogram = handle_nan(processed_spectrogram)\n",
        "        processed_spectrogram = baseline_correction(processed_spectrogram)\n",
        "        processed_spectrogram = apply_notch_filter(processed_spectrogram)\n",
        "        processed_spectrogram = smooth_spectrogram(processed_spectrogram)\n",
        "        processed_spectrogram = normalize_signal(processed_spectrogram)\n",
        "        processed_spectrogram = resample_spectrogram(processed_spectrogram, self.cfg['image_size'])\n",
        "\n",
        "        processed_spectrogram = np.tile(processed_spectrogram[..., None], (1, 1, 3))\n",
        "        if self.plot:\n",
        "            plot_spectrograms(basic_spectrogram, processed_spectrogram, self.train_ids.index.tolist(), num_labels=10)\n",
        "\n",
        "        if self.augmentations:\n",
        "            processed_spectrogram = (processed_spectrogram * 255).astype(np.uint8)\n",
        "            augmented = self.augmentations(image=processed_spectrogram)\n",
        "            processed_spectrogram = augmented['image']\n",
        "            processed_spectrogram = processed_spectrogram.float() / 255.0\n",
        "        else:\n",
        "            processed_spectrogram = processed_spectrogram.astype(np.float32)\n",
        "            processed_spectrogram = torch.tensor(processed_spectrogram).permute(2, 0, 1).float()\n",
        "\n",
        "        return processed_spectrogram, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the train dataset: 85649\n",
            "Size of the valid dataset: 21529\n"
          ]
        }
      ],
      "source": [
        "spec_train_dataset = HMS_Spectrogram_Dataset(train_metadata, cfg=CFG)\n",
        "print(f\"Size of the train dataset: {len(spec_train_dataset)}\")\n",
        "\n",
        "spec_valid_dataset = HMS_Spectrogram_Dataset(valid_metadata, cfg=CFG)\n",
        "print(f\"Size of the valid dataset: {len(spec_valid_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "spec_train_loader = DataLoader(spec_train_dataset, batch_size=CFG['batch_size'], shuffle=True, num_workers=CFG['num_workers'], pin_memory=True)\n",
        "spec_valid_loader = DataLoader(spec_valid_dataset, batch_size=CFG['batch_size'], shuffle=True, num_workers=CFG['num_workers'], pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Iterate over batches and log the shape of the final DataLoader object\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m spec_train_loader:\n\u001b[1;32m      4\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Logs batch size and dimensions\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m/usr/lib/python3/dist-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
            "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "# Iterate over batches and log the shape of the final DataLoader object\n",
        "for batch in spec_train_loader:\n",
        "    data, labels = batch\n",
        "    print(f\"Batch data shape: {data.shape}\")  # Logs batch size and dimensions\n",
        "    print(f\"Batch labels shape: {labels.shape}\")  # Logs label dimensions\n",
        "    break\n",
        "end_time = time.time()\n",
        "duration = end_time - start_time\n",
        "print(f\"took {duration} seconds to display dataloader info.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpectrogramViT(nn.Module):\n",
        "    def __init__(self, image_size=(400, 300), num_classes=6):\n",
        "        super(SpectrogramViT, self).__init__()\n",
        "        \n",
        "        # Initialize a pre-trained Vision Transformer with ImageNet weights\n",
        "        self.vit = vit_b_16(pretrained=True)\n",
        "        \n",
        "        # Calculate the number of patches based on the new image size\n",
        "        new_patch_size = (image_size[0] // 16, image_size[1] // 16)\n",
        "        num_patches = new_patch_size[0] * new_patch_size[1]\n",
        "        \n",
        "        # Adjust the positional embeddings to match the new number of patches\n",
        "        self.vit.encoder.positional_embedding = nn.Parameter(\n",
        "            torch.randn(1, num_patches + 1, 768)\n",
        "        )\n",
        "        \n",
        "        # Get the input features from the classifier head\n",
        "        in_features = self.vit.heads[0].in_features\n",
        "        \n",
        "        # Adjust the classification head to output logits for KLDivLoss\n",
        "        self.vit.heads = nn.Sequential(\n",
        "            nn.Linear(in_features, num_classes),\n",
        "            nn.LogSoftmax(dim=1)  # Use LogSoftmax for KLDivLoss\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vit(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SpectrogramViT(\n",
            "  (vit): VisionTransformer(\n",
            "    (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (encoder): Encoder(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (layers): Sequential(\n",
            "        (encoder_layer_0): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_1): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_2): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_3): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_4): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_5): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_6): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_7): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_8): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_9): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_10): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (encoder_layer_11): EncoderBlock(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (self_attention): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (mlp): MLPBlock(\n",
            "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Dropout(p=0.0, inplace=False)\n",
            "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (4): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    )\n",
            "    (heads): Sequential(\n",
            "      (0): Linear(in_features=768, out_features=6, bias=True)\n",
            "      (1): LogSoftmax(dim=1)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Create the ViT model\n",
        "model = SpectrogramViT(image_size=(400, 300), num_classes=6)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function adjusted for KL divergence\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training function remains the same, but ensure the labels are in probability form\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data in train_loader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Ensure labels are a probability distribution if using KLDivLoss\n",
        "            labels = labels / labels.sum(dim=1, keepdim=True)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = 100 * correct / total\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
        "\n",
        "        # Validation Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "            for data in val_loader:\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                labels = labels / labels.sum(dim=1, keepdim=True)\n",
        "                \n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
        "\n",
        "            val_loss = val_loss / len(val_loader.dataset)\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    print('Training complete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_model(model, spec_train_loader, spec_valid_loader, criterion, optimizer, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

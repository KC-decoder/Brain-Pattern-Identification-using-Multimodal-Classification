{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1dGzrSs2EycY"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchsummary'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau, StepLR\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m amp\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset, default_collate\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import gc\n",
        "import copy\n",
        "import yaml\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import typing as tp\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from collections import OrderedDict, defaultdict\n",
        "from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import butter, lfilter, freqz, filtfilt,iirnotch\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.optim import Adam, AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
        "from torch.cuda import amp\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset, default_collate\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import v2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import timm\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from skimage.transform import resize\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from captum.attr import Saliency, IntegratedGradients\n",
        "\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.functional import softmax\n",
        "# Generate Segments\n",
        "from skimage.segmentation import slic\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.optim import lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Sp-pb7yEycb",
        "outputId": "8d71912f-cb8c-40a5-c02f-5138caa48747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcgZCTJwEycb"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    debug = True\n",
        "    AUGMENT = False\n",
        "    VALIDATION_FRAC = 0.4\n",
        "    AUGMENTATION_FRACTION = 0.05\n",
        "    EPOCHS = 50\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #checkpoint_dir = \"/Users/koushani/Documents/UB-COURSEWORK/SPRING24/XAI_HMS_KAGGLE/kou/github/Brain-Pattern-Identification-using-Multimodal-Classification/checkpoints/attention_model\"\n",
        "    checkpoint_dir = \"/eng/home/koushani/Documents/Multimodal_XAI/Brain-Pattern-Identification-using-Multimodal-Classification/checkpoint_dir/\"\n",
        "    last_optimizer = None\n",
        "    last_regularization_lambda = None\n",
        "    checkpointing_enabled = True\n",
        "\n",
        "\n",
        "    SKIP_ASSERT = False\n",
        "\n",
        "\n",
        "    seed = 42\n",
        "    gpu_idx = 0\n",
        "\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "    EEG_COLUMNS =  [ # to assert columns order is the same\n",
        "    'Fp1','F3', 'C3', 'P3', 'F7',\n",
        "    'T3', 'T5', 'O1', 'Fz', 'Cz',\n",
        "    'Pz', 'Fp2', 'F4', 'C4', 'P4',\n",
        "    'F8', 'T4', 'T6', 'O2', 'EKG'\n",
        "    ]\n",
        "\n",
        "    eeg_features = [column for column in EEG_COLUMNS if column != 'EKG'] # [ 'EKG' ]\n",
        "\n",
        "    classes = ['Seizure', 'LPD', 'GPD', 'LRDA', 'GRDA', 'Other']\n",
        "    label2name = dict(enumerate(classes))\n",
        "    name2label = {v: k for k, v in label2name.items()}\n",
        "    n_classes = len(classes)\n",
        "\n",
        "    # Spectrogram columns\n",
        "    SPECTR_COLUMNS = [\n",
        "    'time', 'LL_0.59', 'LL_0.78', 'LL_0.98', 'LL_1.17', 'LL_1.37',\n",
        "    'LL_1.56', 'LL_1.76', 'LL_1.95', 'LL_2.15', 'LL_2.34', 'LL_2.54',\n",
        "    'LL_2.73', 'LL_2.93', 'LL_3.13', 'LL_3.32', 'LL_3.52', 'LL_3.71',\n",
        "    'LL_3.91', 'LL_4.1', 'LL_4.3', 'LL_4.49', 'LL_4.69', 'LL_4.88',\n",
        "    'LL_5.08', 'LL_5.27', 'LL_5.47', 'LL_5.66', 'LL_5.86', 'LL_6.05',\n",
        "    'LL_6.25', 'LL_6.45', 'LL_6.64', 'LL_6.84', 'LL_7.03', 'LL_7.23',\n",
        "    'LL_7.42', 'LL_7.62', 'LL_7.81', 'LL_8.01', 'LL_8.2', 'LL_8.4',\n",
        "    'LL_8.59', 'LL_8.79', 'LL_8.98', 'LL_9.18', 'LL_9.38', 'LL_9.57',\n",
        "    'LL_9.77', 'LL_9.96', 'LL_10.16', 'LL_10.35', 'LL_10.55', 'LL_10.74',\n",
        "    'LL_10.94', 'LL_11.13', 'LL_11.33', 'LL_11.52', 'LL_11.72', 'LL_11.91',\n",
        "    'LL_12.11', 'LL_12.3', 'LL_12.5', 'LL_12.7', 'LL_12.89', 'LL_13.09',\n",
        "    'LL_13.28', 'LL_13.48', 'LL_13.67', 'LL_13.87', 'LL_14.06', 'LL_14.26',\n",
        "    'LL_14.45', 'LL_14.65', 'LL_14.84', 'LL_15.04', 'LL_15.23', 'LL_15.43',\n",
        "    'LL_15.63', 'LL_15.82', 'LL_16.02', 'LL_16.21', 'LL_16.41', 'LL_16.6',\n",
        "    'LL_16.8', 'LL_16.99', 'LL_17.19', 'LL_17.38', 'LL_17.58', 'LL_17.77',\n",
        "    'LL_17.97', 'LL_18.16', 'LL_18.36', 'LL_18.55', 'LL_18.75', 'LL_18.95',\n",
        "    'LL_19.14', 'LL_19.34', 'LL_19.53', 'LL_19.73', 'LL_19.92', 'RL_0.59',\n",
        "    'RL_0.78', 'RL_0.98', 'RL_1.17', 'RL_1.37', 'RL_1.56', 'RL_1.76',\n",
        "    'RL_1.95', 'RL_2.15', 'RL_2.34', 'RL_2.54', 'RL_2.73', 'RL_2.93',\n",
        "    'RL_3.13', 'RL_3.32', 'RL_3.52', 'RL_3.71', 'RL_3.91', 'RL_4.1',\n",
        "    'RL_4.3', 'RL_4.49', 'RL_4.69', 'RL_4.88', 'RL_5.08', 'RL_5.27',\n",
        "    'RL_5.47', 'RL_5.66', 'RL_5.86', 'RL_6.05', 'RL_6.25', 'RL_6.45',\n",
        "    'RL_6.64', 'RL_6.84', 'RL_7.03', 'RL_7.23', 'RL_7.42', 'RL_7.62',\n",
        "    'RL_7.81', 'RL_8.01', 'RL_8.2', 'RL_8.4', 'RL_8.59', 'RL_8.79',\n",
        "    'RL_8.98', 'RL_9.18', 'RL_9.38', 'RL_9.57', 'RL_9.77', 'RL_9.96',\n",
        "    'RL_10.16', 'RL_10.35', 'RL_10.55', 'RL_10.74', 'RL_10.94', 'RL_11.13',\n",
        "    'RL_11.33', 'RL_11.52', 'RL_11.72', 'RL_11.91', 'RL_12.11', 'RL_12.3',\n",
        "    'RL_12.5', 'RL_12.7', 'RL_12.89', 'RL_13.09', 'RL_13.28', 'RL_13.48',\n",
        "    'RL_13.67', 'RL_13.87', 'RL_14.06', 'RL_14.26', 'RL_14.45', 'RL_14.65',\n",
        "    'RL_14.84', 'RL_15.04', 'RL_15.23', 'RL_15.43', 'RL_15.63', 'RL_15.82',\n",
        "    'RL_16.02', 'RL_16.21', 'RL_16.41', 'RL_16.6', 'RL_16.8', 'RL_16.99',\n",
        "    'RL_17.19', 'RL_17.38', 'RL_17.58', 'RL_17.77', 'RL_17.97', 'RL_18.16',\n",
        "    'RL_18.36', 'RL_18.55', 'RL_18.75', 'RL_18.95', 'RL_19.14', 'RL_19.34',\n",
        "    'RL_19.53', 'RL_19.73', 'RL_19.92', 'LP_0.59', 'LP_0.78', 'LP_0.98',\n",
        "    'LP_1.17', 'LP_1.37', 'LP_1.56', 'LP_1.76', 'LP_1.95', 'LP_2.15',\n",
        "    'LP_2.34', 'LP_2.54', 'LP_2.73', 'LP_2.93', 'LP_3.13', 'LP_3.32',\n",
        "    'LP_3.52', 'LP_3.71', 'LP_3.91', 'LP_4.1', 'LP_4.3', 'LP_4.49',\n",
        "    'LP_4.69', 'LP_4.88', 'LP_5.08', 'LP_5.27', 'LP_5.47', 'LP_5.66',\n",
        "    'LP_5.86', 'LP_6.05', 'LP_6.25', 'LP_6.45', 'LP_6.64', 'LP_6.84',\n",
        "    'LP_7.03', 'LP_7.23', 'LP_7.42', 'LP_7.62', 'LP_7.81', 'LP_8.01',\n",
        "    'LP_8.2', 'LP_8.4', 'LP_8.59', 'LP_8.79', 'LP_8.98', 'LP_9.18',\n",
        "    'LP_9.38', 'LP_9.57', 'LP_9.77', 'LP_9.96', 'LP_10.16', 'LP_10.35',\n",
        "    'LP_10.55', 'LP_10.74', 'LP_10.94', 'LP_11.13', 'LP_11.33', 'LP_11.52',\n",
        "    'LP_11.72', 'LP_11.91', 'LP_12.11', 'LP_12.3', 'LP_12.5', 'LP_12.7',\n",
        "    'LP_12.89', 'LP_13.09', 'LP_13.28', 'LP_13.48', 'LP_13.67', 'LP_13.87',\n",
        "    'LP_14.06', 'LP_14.26', 'LP_14.45', 'LP_14.65', 'LP_14.84', 'LP_15.04',\n",
        "    'LP_15.23', 'LP_15.43', 'LP_15.63', 'LP_15.82', 'LP_16.02', 'LP_16.21',\n",
        "    'LP_16.41', 'LP_16.6', 'LP_16.8', 'LP_16.99', 'LP_17.19', 'LP_17.38',\n",
        "    'LP_17.58', 'LP_17.77', 'LP_17.97', 'LP_18.16', 'LP_18.36', 'LP_18.55',\n",
        "    'LP_18.75', 'LP_18.95', 'LP_19.14', 'LP_19.34', 'LP_19.53', 'LP_19.73',\n",
        "    'LP_19.92', 'RP_0.59', 'RP_0.78', 'RP_0.98', 'RP_1.17', 'RP_1.37',\n",
        "    'RP_1.56', 'RP_1.76', 'RP_1.95', 'RP_2.15', 'RP_2.34', 'RP_2.54',\n",
        "    'RP_2.73', 'RP_2.93', 'RP_3.13', 'RP_3.32', 'RP_3.52', 'RP_3.71',\n",
        "    'RP_3.91', 'RP_4.1', 'RP_4.3', 'RP_4.49', 'RP_4.69', 'RP_4.88',\n",
        "    'RP_5.08', 'RP_5.27', 'RP_5.47', 'RP_5.66', 'RP_5.86', 'RP_6.05',\n",
        "    'RP_6.25', 'RP_6.45', 'RP_6.64', 'RP_6.84', 'RP_7.03', 'RP_7.23',\n",
        "    'RP_7.42', 'RP_7.62', 'RP_7.81', 'RP_8.01', 'RP_8.2', 'RP_8.4',\n",
        "    'RP_8.59', 'RP_8.79', 'RP_8.98', 'RP_9.18', 'RP_9.38', 'RP_9.57',\n",
        "    'RP_9.77', 'RP_9.96', 'RP_10.16', 'RP_10.35', 'RP_10.55', 'RP_10.74',\n",
        "    'RP_10.94', 'RP_11.13', 'RP_11.33', 'RP_11.52', 'RP_11.72', 'RP_11.91',\n",
        "    'RP_12.11', 'RP_12.3', 'RP_12.5', 'RP_12.7', 'RP_12.89', 'RP_13.09',\n",
        "    'RP_13.28', 'RP_13.48', 'RP_13.67', 'RP_13.87', 'RP_14.06', 'RP_14.26',\n",
        "    'RP_14.45', 'RP_14.65', 'RP_14.84', 'RP_15.04', 'RP_15.23', 'RP_15.43',\n",
        "    'RP_15.63', 'RP_15.82', 'RP_16.02', 'RP_16.21', 'RP_16.41', 'RP_16.6',\n",
        "    'RP_16.8', 'RP_16.99', 'RP_17.19', 'RP_17.38', 'RP_17.58', 'RP_17.77',\n",
        "    'RP_17.97', 'RP_18.16', 'RP_18.36', 'RP_18.55', 'RP_18.75', 'RP_18.95',\n",
        "    'RP_19.14', 'RP_19.34', 'RP_19.53', 'RP_19.73', 'RP_19.92'\n",
        "]\n",
        "\n",
        "\n",
        "    n_folds = 5\n",
        "    SPECTROGRAM_COLUMNS = [col for col in SPECTR_COLUMNS if col != 'time']\n",
        "\n",
        "    ## Dataset Preprocessing\n",
        "    bandpass_filter = {\"low\": 0.5, \"high\": 20, \"order\": 2}\n",
        "    rand_filter = {\"probab\": 0.1, \"low\": 10, \"high\": 20, \"band\": 1.0, \"order\": 2}\n",
        "    freq_channels = [(0.5, 4.5)]\n",
        "    filter_order = 2\n",
        "    random_close_zone = 0.0  # 0.2\n",
        "\n",
        "    map_features = [\n",
        "        (\"Fp1\", \"F7\"), (\"F7\", \"T3\"), (\"T3\", \"T5\"), (\"T5\", \"O1\"),\n",
        "        (\"Fp1\", \"F3\"), (\"F3\", \"C3\"), (\"C3\", \"P3\"), (\"P3\", \"O1\"),\n",
        "        (\"Fp2\", \"F8\"), (\"F8\", \"T4\"), (\"T4\", \"T6\"), (\"T6\", \"O2\"),\n",
        "        (\"Fp2\", \"F4\"), (\"F4\", \"C4\"), (\"C4\", \"P4\"), (\"P4\", \"O2\"),\n",
        "        (\"Fz\", \"Cz\"), (\"Cz\", \"Pz\")\n",
        "    ]\n",
        "\n",
        "    LL = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1']  # Example left EEG channels\n",
        "    LP = ['Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2']  # Example right EEG channels\n",
        "    RL = LL  # Assuming RL and RP are mirrors of LL and LP respectively\n",
        "    RP = LP\n",
        "\n",
        "    feature_to_index = {x: y for x, y in zip(eeg_features, range(len(eeg_features)))}\n",
        "    simple_features = ['Fz', 'Cz', 'Pz', 'EKG']  # 'Fz', 'Cz', 'Pz', 'EKG'\n",
        "\n",
        "    n_map_features = len(map_features)\n",
        "    in_channels = len(SPECTROGRAM_COLUMNS)\n",
        "\n",
        "    seq_length = 50  # Second's\n",
        "    sampling_rate = 200  # Hz\n",
        "    nsamples = seq_length * sampling_rate\n",
        "    out_samples = nsamples // 5\n",
        "\n",
        "    debug_input_size = 4096\n",
        "    #input_size =\n",
        "    in_chans = 1\n",
        "\n",
        "    batch_size = 1024\n",
        "    num_workers = 12\n",
        "    fixed_length = 3000\n",
        "    image_size = (400, 300)\n",
        "\n",
        "    def seed_everything(self):\n",
        "        np.random.seed(self.seed)\n",
        "        torch.manual_seed(self.seed)\n",
        "        random.seed(self.seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed(self.seed)\n",
        "            torch.cuda.manual_seed_all(self.seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    def get_device():\n",
        "        if torch.cuda.is_available():\n",
        "        # Get the current default CUDA device\n",
        "            device = torch.device(\"cuda\")\n",
        "        # Get the name of the device\n",
        "            device_name = torch.cuda.get_device_name(device)\n",
        "            print(f\"CUDA is available. Using device: {device} ({device_name})\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"CUDA is not available. Using CPU.\")\n",
        "        return device\n",
        "\n",
        "\n",
        "    def stop_checkpointing():\n",
        "        CFG.checkpointing_enabled = False\n",
        "        print(\"Checkpointing disabled.\")\n",
        "\n",
        "    def start_checkpointing():\n",
        "        CFG.checkpointing_enabled = True\n",
        "        print(\"Checkpointing enabled.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT6T5NeDEycb"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        valid_losses = checkpoint['valid_losses']\n",
        "        train_accuracies = checkpoint['train_accuracies']\n",
        "        valid_accuracies = checkpoint['valid_accuracies']\n",
        "        lr_scheduler = checkpoint['lr_scheduler']\n",
        "        regularization_losses = checkpoint['regularization_losses']\n",
        "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}'\")\n",
        "        start_epoch = 0\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        train_accuracies = []\n",
        "        valid_accuracies = []\n",
        "        lr_scheduler = []\n",
        "        regularization_losses = []\n",
        "\n",
        "    return start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies, lr_scheduler, regularization_losses\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def save_checkpoint(state, checkpoint_dir, checkpoint_filename):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    torch.save(state, checkpoint_path)\n",
        "    print(f\"Checkpoint saved at '{checkpoint_path}'\")\n",
        "\n",
        "\n",
        "def detect_and_save_checkpoint(state, checkpoint_dir, optimizer, regularization_lambda):\n",
        "    # Detect changes in optimizer and regularization parameter\n",
        "    optimizer_changed = CFG.last_optimizer is None or type(optimizer) != CFG.last_optimizer\n",
        "    regularization_changed = CFG.last_regularization_lambda is None or regularization_lambda != CFG.last_regularization_lambda\n",
        "\n",
        "    # Initialize the checkpoint filename\n",
        "    checkpoint_filename = \"checkpoint.pth.tar\"\n",
        "\n",
        "    # Modify the checkpoint filename based on the changes detected\n",
        "    if optimizer_changed and regularization_changed:\n",
        "        checkpoint_filename = \"checkpoint_optimizer_and_regularization.pth.tar\"\n",
        "    elif optimizer_changed:\n",
        "        checkpoint_filename = \"checkpoint_optimizer.pth.tar\"\n",
        "    elif regularization_changed:\n",
        "        checkpoint_filename = \"checkpoint_regularization.pth.tar\"\n",
        "\n",
        "    if optimizer_changed or regularization_changed:\n",
        "        print(f\"Changes detected in {'optimizer' if optimizer_changed else ''} {'and' if optimizer_changed and regularization_changed else ''} {'regularization parameter' if regularization_changed else ''}. Creating a new checkpoint.\")\n",
        "        CFG.last_optimizer = type(optimizer)\n",
        "        CFG.last_regularization_lambda = regularization_lambda\n",
        "        save_checkpoint(state, checkpoint_dir, checkpoint_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgK9iBIzEycc",
        "outputId": "f5af5326-b359-41a4-f1a0-f1d1e6b2e47d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available. Using device: cuda (NVIDIA A100-SXM4-40GB)\n"
          ]
        }
      ],
      "source": [
        "CFG.device = CFG.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du34Um7fEycc"
      },
      "outputs": [],
      "source": [
        "#ROOT_DIR = '/Users/koushani/Documents/UB-COURSEWORK/SPRING24/XAI_HMS_KAGGLE/CODING/DATA'\n",
        "ROOT_DIR = '/eng/home/koushani/Documents/Multimodal_XAI/HMS_data'\n",
        "TRAIN_EEGS = os.path.join(ROOT_DIR , 'train_eegs')\n",
        "TRAIN_SPECTR =  os.path.join(ROOT_DIR, 'train_spectrograms')\n",
        "TEST_EEGS = os.path.join(ROOT_DIR, 'test_eegs')\n",
        "TEST_SPECTR = os.path.join(ROOT_DIR, 'test_spectrograms')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGncovR0Eycc"
      },
      "outputs": [],
      "source": [
        "def plot_eeg_data_comparison(original_data, processed_data, cfg, title_original='Original EEG Data', title_processed='Processed EEG Data'):\n",
        "    \"\"\"\n",
        "    Correctly aligns and plots original, processed, and differential EEG data.\n",
        "\n",
        "    Parameters:\n",
        "    - original_data: numpy array of shape (original_channels, original_samples)\n",
        "    - processed_data: numpy array of shape (processed_channels, processed_samples) including differential channels\n",
        "    - cfg: configuration object with channel names and mappings\n",
        "    - title_original: title for the original data plot\n",
        "    - title_processed: title for the processed data plot\n",
        "    \"\"\"\n",
        "    original_channels, original_samples = original_data.shape\n",
        "    processed_channels = cfg.in_channels  # Assuming this reflects the primary EEG channels without differentials\n",
        "    differential_channels = len(cfg.map_features)  # Number of differential channels\n",
        "\n",
        "    total_channels = original_channels + differential_channels  # Total number of channels to plot\n",
        "    fig, axes = plt.subplots(total_channels, 2, figsize=(15, 2 * total_channels), sharex='col')\n",
        "    fig.suptitle('EEG Data Comparison')\n",
        "\n",
        "    # Plot the original channels on the left\n",
        "    for i in range(original_channels):\n",
        "        axes[i, 0].plot(original_data[i], color='b')\n",
        "        axes[i, 0].set_ylabel(cfg.EEG_COLUMNS[i])\n",
        "        if i == 0:\n",
        "            axes[i, 0].set_title(title_original)\n",
        "\n",
        "    # Plot the primary processed channels on the right\n",
        "    for i in range(processed_channels):\n",
        "        axes[i, 1].plot(processed_data[i], color='b')\n",
        "        axes[i, 1].set_ylabel(cfg.eeg_features[i])\n",
        "        if i == 0:\n",
        "            axes[i, 1].set_title(title_processed)\n",
        "\n",
        "    # Plot differential signals below the primary channels\n",
        "    differential_start = original_channels  # Starting index for differential signals\n",
        "    for j in range(differential_channels):\n",
        "        k = differential_start + j\n",
        "        if k < total_channels:\n",
        "            axes[k, 1].plot(processed_data[processed_channels + j], color='orange')\n",
        "            axes[k, 1].set_ylabel(f'{cfg.map_features[j][0]}-{cfg.map_features[j][1]}')\n",
        "\n",
        "    # Ensure labels and axes are correctly shown\n",
        "    for ax_row in axes:\n",
        "        for ax in ax_row:\n",
        "            ax.set_xlabel('Samples')\n",
        "            if not ax.lines:\n",
        "                ax.axis('off')  # Turn off empty plots\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMRWiT9zEycc"
      },
      "outputs": [],
      "source": [
        "def plot_training_results(num_epochs, train_losses=None, valid_losses=None, train_accuracies=None, valid_accuracies=None, checkpoint_file=None):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss, accuracy, and learning rate over the given number of epochs.\n",
        "\n",
        "    Args:\n",
        "    - num_epochs (int): The number of epochs.\n",
        "    - train_losses (list): List of training losses.\n",
        "    - valid_losses (list): List of validation losses.\n",
        "    - train_accuracies (list): List of training accuracies.\n",
        "    - valid_accuracies (list): List of validation accuracies.\n",
        "    - checkpoint_file (str): Path to the checkpoint file to load data from.\n",
        "    \"\"\"\n",
        "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
        "        checkpoint = torch.load(checkpoint_file)\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "        valid_losses = checkpoint.get('valid_losses', [])\n",
        "        train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "        valid_accuracies = checkpoint.get('valid_accuracies', [])\n",
        "        lr_scheduler = checkpoint.get('lr_scheduler', [])\n",
        "        num_epochs = CFG.EPOCHS\n",
        "\n",
        "    epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # Plotting Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    if train_losses and valid_losses:\n",
        "        plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "        plt.plot(epochs_range, valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Plotting Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if train_accuracies and valid_accuracies:\n",
        "        plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
        "        plt.plot(epochs_range, valid_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Plotting Learning Rate\n",
        "    plt.subplot(1, 3, 3)\n",
        "    if lr_scheduler:\n",
        "        plt.plot(epochs_range, lr_scheduler, label='Learning Rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    plt.title('Learning Rate Schedule')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdhEI7TIEycc"
      },
      "outputs": [],
      "source": [
        "# Function to check if the EEG data is entirely NaN\n",
        "def is_entirely_nan(eeg_id):\n",
        "    eeg_data = load_train_eeg_frame(eeg_id)\n",
        "    return np.isnan(eeg_data.values).all()\n",
        "\n",
        "def load_train_eeg_frame(id):\n",
        "    # Ensure the ID is an integer to avoid issues with file name construction\n",
        "    id = int(id)\n",
        "    # Construct the file path using the integer ID\n",
        "    file_path = os.path.join(TRAIN_EEGS, f'{id}.parquet')\n",
        "    # Load the EEG data from the specified Parquet file\n",
        "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
        "    # Optional: Verify that the columns match expected EEG columns\n",
        "    if not CFG.SKIP_ASSERT:\n",
        "        assert list(data.columns) == CFG.EEG_COLUMNS, 'EEG columns order is not the same!'\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_train_spectr_frame(id):\n",
        "    # Ensure the ID is an integer to prevent file path errors\n",
        "    id = int(id)\n",
        "    # Construct the file path using the integer ID\n",
        "    file_path = os.path.join(TRAIN_SPECTR, f'{id}.parquet')\n",
        "    # Load the spectrogram data from the specified Parquet file\n",
        "    data = pd.read_parquet(file_path, engine='pyarrow')\n",
        "    # Optional: Verify that the columns match expected Spectrogram columns\n",
        "    if not CFG.SKIP_ASSERT:\n",
        "        assert list(data.columns) == CFG.SPECTR_COLUMNS, 'Spectrogram columns order is not the same!'\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaV_2tB7Eycd"
      },
      "outputs": [],
      "source": [
        "def create_reference_data_loader(dataset, batch_size, device):\n",
        "    data_loader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "    reference_data = []\n",
        "    with tqdm(data_loader, desc=\"Generating reference data\", unit=\"batch\") as tepoch:\n",
        "        for spectrogram, label in tepoch:\n",
        "            spectrogram, label = spectrogram.to(CFG.device), label.to(CFG.device)  # Move to GPU\n",
        "            for i in range(spectrogram.size(0)):\n",
        "                image = spectrogram[i].cpu().numpy().transpose(1, 2, 0)  # Move back to CPU for albumentations\n",
        "                lbl = label[i].cpu().numpy()\n",
        "                reference_data.append({\"image\": image, \"global_label\": lbl})\n",
        "    return reference_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TXflXjQEycd"
      },
      "outputs": [],
      "source": [
        "def get_augmentations(reference_data):\n",
        "    return A.Compose([\n",
        "        A.MixUp(reference_data=reference_data, read_fn=lambda x: x, p=0.5),\n",
        "        A.CoarseDropout(max_holes=1, min_height=1.0, max_height=1.0,\n",
        "                        min_width=0.06, max_width=0.1, p=0.5),  # freq-masking\n",
        "        A.CoarseDropout(max_holes=1, min_height=0.06, max_height=0.1,\n",
        "                        min_width=1.0, max_width=1.0, p=0.5),  # time-masking\n",
        "        ToTensorV2()\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fTFjDCbEycd"
      },
      "outputs": [],
      "source": [
        "def create_k_fold_splits(metadata, n_splits=5):\n",
        "    # Drop unnecessary columns\n",
        "    metadata.drop(columns=[\n",
        "            'eeg_sub_id',\n",
        "            'spectrogram_sub_id',\n",
        "            'patient_id',\n",
        "            'label_id'\n",
        "        ], inplace=True)\n",
        "\n",
        "    # Ensure correct data types\n",
        "    metadata['eeg_id'] = metadata['eeg_id'].astype(int)\n",
        "    metadata['spectrogram_id'] = metadata['spectrogram_id'].astype(int)\n",
        "    metadata['eeg_label_offset_seconds'] = metadata['eeg_label_offset_seconds'].astype(int)\n",
        "    metadata['spectrogram_label_offset_seconds'] = metadata['spectrogram_label_offset_seconds'].astype(int)\n",
        "\n",
        "    # Debugging: Sample the data if in DEBUG mode to reduce size for faster processing\n",
        "    if CFG.debug:\n",
        "        metadata = metadata.sample(min(CFG.debug_input_size, len(metadata)))\n",
        "\n",
        "    # Extract features and labels for stratification\n",
        "    X = metadata['eeg_id']\n",
        "    y = metadata['expert_consensus']  # Correct column name for class labels\n",
        "\n",
        "    # Create stratified K-Folds\n",
        "    skf = StratifiedKFold(n_splits=n_splits)\n",
        "    fold_indices = []\n",
        "\n",
        "    for train_index, valid_index in skf.split(X, y):\n",
        "        train_ids = X.iloc[train_index].tolist()\n",
        "        valid_ids = X.iloc[valid_index].tolist()\n",
        "        fold_indices.append((train_ids, valid_ids))\n",
        "\n",
        "    return fold_indices\n",
        "\n",
        "def createTrainTestSplit(metadata, fold_indices, fold_idx):\n",
        "    train_ids, valid_ids = fold_indices[fold_idx]\n",
        "\n",
        "    train_metadata = metadata[metadata['eeg_id'].isin(train_ids)]\n",
        "    valid_metadata = metadata[metadata['eeg_id'].isin(valid_ids)]\n",
        "\n",
        "    if CFG.AUGMENT:\n",
        "        # Select 10% of train_metadata for augmentation\n",
        "        augmentation_size = int(len(train_metadata) * CFG.AUGMENTATION_FRACTION)\n",
        "        augmentation_metadata = train_metadata.sample(augmentation_size)\n",
        "        return train_metadata, valid_metadata, augmentation_metadata\n",
        "\n",
        "    return train_metadata, valid_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "UplIR9-KEycd",
        "outputId": "4e5a6b78-7194-4461-dd04-70d7e164b18f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"main_metadata\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"eeg_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 351917269,\n        \"max\": 351917269,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          351917269\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eeg_sub_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 6,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eeg_label_offset_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1622776601683795,\n        \"min\": 12.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spectrogram_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2147388374,\n        \"max\": 2147388374,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2147388374\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spectrogram_sub_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 6,\n        \"max\": 10,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"spectrogram_label_offset_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1622776601683795,\n        \"min\": 12.0,\n        \"max\": 20.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          14.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1927973036,\n        \"min\": 290896675,\n        \"max\": 4195677307,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          290896675\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patient_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 10351,\n        \"max\": 10351,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10351\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expert_consensus\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"LRDA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seizure_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lpd_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gpd_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lrda_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"grda_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"other_vote\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2f537781-83e3-4131-8ae3-d064e7bab7b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eeg_id</th>\n",
              "      <th>eeg_sub_id</th>\n",
              "      <th>eeg_label_offset_seconds</th>\n",
              "      <th>spectrogram_id</th>\n",
              "      <th>spectrogram_sub_id</th>\n",
              "      <th>spectrogram_label_offset_seconds</th>\n",
              "      <th>label_id</th>\n",
              "      <th>patient_id</th>\n",
              "      <th>expert_consensus</th>\n",
              "      <th>seizure_vote</th>\n",
              "      <th>lpd_vote</th>\n",
              "      <th>gpd_vote</th>\n",
              "      <th>lrda_vote</th>\n",
              "      <th>grda_vote</th>\n",
              "      <th>other_vote</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106795</th>\n",
              "      <td>351917269</td>\n",
              "      <td>6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>2147388374</td>\n",
              "      <td>6</td>\n",
              "      <td>12.0</td>\n",
              "      <td>4195677307</td>\n",
              "      <td>10351</td>\n",
              "      <td>LRDA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106796</th>\n",
              "      <td>351917269</td>\n",
              "      <td>7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2147388374</td>\n",
              "      <td>7</td>\n",
              "      <td>14.0</td>\n",
              "      <td>290896675</td>\n",
              "      <td>10351</td>\n",
              "      <td>LRDA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106797</th>\n",
              "      <td>351917269</td>\n",
              "      <td>8</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2147388374</td>\n",
              "      <td>8</td>\n",
              "      <td>16.0</td>\n",
              "      <td>461435451</td>\n",
              "      <td>10351</td>\n",
              "      <td>LRDA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106798</th>\n",
              "      <td>351917269</td>\n",
              "      <td>9</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2147388374</td>\n",
              "      <td>9</td>\n",
              "      <td>18.0</td>\n",
              "      <td>3786213131</td>\n",
              "      <td>10351</td>\n",
              "      <td>LRDA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106799</th>\n",
              "      <td>351917269</td>\n",
              "      <td>10</td>\n",
              "      <td>20.0</td>\n",
              "      <td>2147388374</td>\n",
              "      <td>10</td>\n",
              "      <td>20.0</td>\n",
              "      <td>3642716176</td>\n",
              "      <td>10351</td>\n",
              "      <td>LRDA</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f537781-83e3-4131-8ae3-d064e7bab7b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2f537781-83e3-4131-8ae3-d064e7bab7b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2f537781-83e3-4131-8ae3-d064e7bab7b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-173634bc-da21-42b4-ac3e-2ea3f48a3b8d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-173634bc-da21-42b4-ac3e-2ea3f48a3b8d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-173634bc-da21-42b4-ac3e-2ea3f48a3b8d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
              "106795  351917269           6                      12.0      2147388374   \n",
              "106796  351917269           7                      14.0      2147388374   \n",
              "106797  351917269           8                      16.0      2147388374   \n",
              "106798  351917269           9                      18.0      2147388374   \n",
              "106799  351917269          10                      20.0      2147388374   \n",
              "\n",
              "        spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
              "106795                   6                              12.0  4195677307   \n",
              "106796                   7                              14.0   290896675   \n",
              "106797                   8                              16.0   461435451   \n",
              "106798                   9                              18.0  3786213131   \n",
              "106799                  10                              20.0  3642716176   \n",
              "\n",
              "        patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  \\\n",
              "106795       10351             LRDA             0         0         0   \n",
              "106796       10351             LRDA             0         0         0   \n",
              "106797       10351             LRDA             0         0         0   \n",
              "106798       10351             LRDA             0         0         0   \n",
              "106799       10351             LRDA             0         0         0   \n",
              "\n",
              "        lrda_vote  grda_vote  other_vote  \n",
              "106795          3          0           0  \n",
              "106796          3          0           0  \n",
              "106797          3          0           0  \n",
              "106798          3          0           0  \n",
              "106799          3          0           0  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#main_metadata = pd.read_csv('/Users/koushani/Documents/UB-COURSEWORK/SPRING24/XAI_HMS_KAGGLE/CODING/DATA/train.csv')\n",
        "main_metadata = pd.read_csv('/eng/home/koushani/Documents/Multimodal_XAI/HMS_data/train.csv')\n",
        "#main_metadata = pd.read_csv('HMS_Data/train.csv')\n",
        "# Check the data types\n",
        "main_metadata.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyADXN8CEycd"
      },
      "outputs": [],
      "source": [
        "# Define the baseline correction function\n",
        "def baseline_correction(sig):\n",
        "    sig -= np.mean(sig, axis=0)\n",
        "    return sig\n",
        "\n",
        "def normalize_signal(sig):\n",
        "    \"\"\"Normalize the signal by scaling it to the range [0, 1], handling NaN values.\"\"\"\n",
        "    # Replace NaN values with the mean of the array\n",
        "    sig = np.nan_to_num(sig, nan=np.nanmean(sig))\n",
        "    # Normalize the signal\n",
        "    return (sig - np.min(sig)) / (np.max(sig) - np.min(sig) + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "\n",
        "# Apply a notch filter to remove specific frequency noise\n",
        "def apply_notch_filter(sig, freq=60, fs=200, quality=30):\n",
        "    b, a = iirnotch(freq, quality, fs)\n",
        "    sig = filtfilt(b, a, sig, axis=0)\n",
        "    return sig\n",
        "\n",
        "# Apply Gaussian smoothing\n",
        "def smooth_spectrogram(sig, sigma=1.0):\n",
        "    sig = gaussian_filter(sig, sigma=sigma)\n",
        "    return sig\n",
        "\n",
        "# Resample the spectrogram\n",
        "def resample_spectrogram(sig, target_shape):\n",
        "    sig = resize(sig, target_shape, mode='reflect', anti_aliasing=True)\n",
        "    return sig\n",
        "\n",
        "\n",
        "def labels_to_probabilities(labels, num_classes):\n",
        "        labels = torch.eye(num_classes)[labels]\n",
        "        return labels\n",
        "\n",
        "\n",
        "def handle_nan(data):\n",
        "    \"\"\"Handle NaN values by replacing them with the mean of the respective channels.\"\"\"\n",
        "    # Replace NaN values with the mean of each column\n",
        "    mean_values = np.nanmean(data, axis=0)\n",
        "    inds = np.where(np.isnan(data))\n",
        "    data[inds] = np.take(mean_values, inds[1])\n",
        "    return data\n",
        "\n",
        "def pad_or_truncate(data, target_shape):\n",
        "    target_rows, target_cols = target_shape\n",
        "\n",
        "    # Pad or truncate the number of rows\n",
        "    if data.shape[0] < target_rows:\n",
        "        row_padding = np.zeros((target_rows - data.shape[0], data.shape[1]))\n",
        "        data = np.vstack((data, row_padding))\n",
        "    else:\n",
        "        data = data[:target_rows, :]\n",
        "\n",
        "    # Pad or truncate the number of columns\n",
        "    if data.shape[1] < target_cols:\n",
        "        col_padding = np.zeros((data.shape[0], target_cols - data.shape[1]))\n",
        "        data = np.hstack((data, col_padding))\n",
        "    else:\n",
        "        data = data[:, :target_cols]\n",
        "\n",
        "    return data\n",
        "\n",
        "def plot_spectrograms(raw, processed, labels, num_labels):\n",
        "    x_ticks = np.linspace(0, processed.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = [labels[i] for i in x_ticks]\n",
        "\n",
        "    plt.figure(figsize=(40, 16))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Raw Signal\")\n",
        "    plt.imshow(raw, aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Processed Signal\")\n",
        "    if processed.ndim == 3 and processed.shape[2] > 1:\n",
        "        plt.imshow(processed[:, :, 0], aspect='auto', cmap='viridis')\n",
        "    else:\n",
        "        plt.imshow(processed.squeeze(), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTLxnQesEyce"
      },
      "outputs": [],
      "source": [
        "class HMS_Spectrogram_Dataset(Dataset):\n",
        "    def __init__(self, train_ids, cfg, augmentations=None, plot=False):\n",
        "        super(HMS_Spectrogram_Dataset, self).__init__()\n",
        "        self.train_ids = train_ids\n",
        "        self.cfg = cfg\n",
        "        self.augmentations = augmentations\n",
        "        self.plot = plot  # Flag to control plotting\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.train_ids.iloc[idx]\n",
        "        spec_id = row['spectrogram_id']\n",
        "        raw_spectrogram = load_train_spectr_frame(spec_id)\n",
        "        label_name = row['expert_consensus']\n",
        "        label_idx = self.cfg.name2label[label_name]\n",
        "        label = labels_to_probabilities(label_idx, self.cfg.n_classes)\n",
        "        offset = row.get(\"spectrogram_label_offset_seconds\", None)\n",
        "\n",
        "        if isinstance(raw_spectrogram, pd.DataFrame):\n",
        "            raw_spectrogram = raw_spectrogram.to_numpy()\n",
        "\n",
        "        if offset is not None:\n",
        "            offset = offset // 2\n",
        "            basic_spectrogram = raw_spectrogram[:, offset:offset + 300]\n",
        "            pad_size = max(0, 300 - basic_spectrogram.shape[1])\n",
        "            basic_spectrogram = np.pad(basic_spectrogram, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        else:\n",
        "            basic_spectrogram = raw_spectrogram\n",
        "\n",
        "        spectrogram = basic_spectrogram.T\n",
        "        processed_spectrogram = pad_or_truncate(spectrogram, self.cfg.image_size)\n",
        "        processed_spectrogram = handle_nan(processed_spectrogram)\n",
        "        processed_spectrogram = baseline_correction(processed_spectrogram)\n",
        "        processed_spectrogram = apply_notch_filter(processed_spectrogram)\n",
        "        processed_spectrogram = smooth_spectrogram(processed_spectrogram)\n",
        "        processed_spectrogram = normalize_signal(processed_spectrogram)\n",
        "        processed_spectrogram = resample_spectrogram(processed_spectrogram, self.cfg.image_size)\n",
        "\n",
        "        processed_spectrogram = np.tile(processed_spectrogram[..., None], (1, 1, 3))\n",
        "        if self.plot:\n",
        "            plot_spectrograms(basic_spectrogram, processed_spectrogram, self.train_ids.index.tolist(), num_labels=10)\n",
        "\n",
        "        if self.augmentations:\n",
        "            processed_spectrogram = (processed_spectrogram * 255).astype(np.uint8)\n",
        "            augmented = self.augmentations(image=processed_spectrogram)\n",
        "            processed_spectrogram = augmented['image']\n",
        "            processed_spectrogram = processed_spectrogram.float() / 255.0\n",
        "        else:\n",
        "            processed_spectrogram = processed_spectrogram.astype(np.float32)\n",
        "            processed_spectrogram = torch.tensor(processed_spectrogram).permute(2, 0, 1).float()\n",
        "\n",
        "\n",
        "\n",
        "        if not isinstance(label, torch.Tensor):\n",
        "            label = torch.tensor(label, dtype=torch.float32)\n",
        "        else:\n",
        "            label = label.float()\n",
        "\n",
        "        return processed_spectrogram, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPYzh5_uEyce"
      },
      "outputs": [],
      "source": [
        "class HMS_EEG_Dataset(Dataset):\n",
        "    def __init__(self, train_ids, cfg=CFG, training_flag=False, shuffle=False):\n",
        "        super(HMS_EEG_Dataset, self).__init__()\n",
        "        self.train_ids = train_ids\n",
        "        self.cfg = cfg\n",
        "        self.training_flag = training_flag\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Set the random seed for reproducibility\n",
        "        self.cfg.seed_everything(CFG)\n",
        "        # Feature to index mapping from CFG\n",
        "        self.feature_to_index = self.cfg.feature_to_index\n",
        "        self.differential_channels_start_index = len(self.cfg.feature_to_index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.train_ids.iloc[idx]\n",
        "        data = self.single_map_func(row, self.training_flag)\n",
        "        label_name = row['expert_consensus']  # Assuming 'expert_consensus' column contains the label names\n",
        "        label_idx = self.cfg.name2label[label_name]\n",
        "        label = self.labels_to_probabilities(label_idx, self.cfg.n_classes)\n",
        "        data = data[np.newaxis, ...]  # Add channel dimension for EEG data, now shape is [1, 37, 3000]\n",
        "        return data.astype(np.float32), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_ids)\n",
        "\n",
        "    def single_map_func(self, row, is_training):\n",
        "        data = self.get_eeg(row, is_training)\n",
        "        data = self.handle_nan(data)\n",
        "        if data.size == 0:\n",
        "            # Skip this sample if data is empty\n",
        "            return np.zeros((self.cfg.in_channels + len(self.cfg.map_features), self.cfg.fixed_length)), np.zeros(self.cfg.n_classes)\n",
        "        data = self.calculate_differential_signals(data)\n",
        "        data = self.denoise_filter(data)\n",
        "        data = self.normalize(data)\n",
        "        data = self.select_and_map_channels(data, self.cfg.eeg_features, self.feature_to_index)\n",
        "        data = self.pad_or_truncate(data, self.cfg.fixed_length)\n",
        "        return data\n",
        "\n",
        "    def get_eeg(self, row, is_training, flip=False):\n",
        "        eeg_id = row['eeg_id']\n",
        "        eeg = load_train_eeg_frame(eeg_id)\n",
        "\n",
        "        waves = eeg.values.T\n",
        "\n",
        "        if CFG.AUGMENT:\n",
        "            waves = self.mirror_eeg(waves)\n",
        "\n",
        "        waves = self.butter_bandpass_filter(waves, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate)\n",
        "\n",
        "        return waves\n",
        "\n",
        "    def handle_nan(self, data):\n",
        "        \"\"\"Handle NaN values by replacing them with the mean of the respective channels.\"\"\"\n",
        "        # Remove rows that are entirely NaN\n",
        "        data = data[~np.isnan(data).all(axis=1)]\n",
        "\n",
        "        # Check if data is empty after removing NaN rows\n",
        "        if data.size == 0:\n",
        "            # Handle the case where all data is NaN, e.g., by filling with zeros or skipping this sample\n",
        "            data = np.zeros((self.cfg.in_channels + len(self.cfg.map_features), self.cfg.fixed_length))\n",
        "        else:\n",
        "            where_nan = np.isnan(data)\n",
        "            mean_values = np.nanmean(data, axis=1, keepdims=True)\n",
        "\n",
        "            # Check for channels where the mean could not be computed and replace NaN means with zeros\n",
        "            mean_values[np.isnan(mean_values)] = 0\n",
        "            data[where_nan] = np.take(mean_values, np.where(where_nan)[0])\n",
        "\n",
        "        return data\n",
        "\n",
        "    def pad_or_truncate(self, data, length):\n",
        "        if data.shape[1] < length:\n",
        "            # Pad with zeros if shorter\n",
        "            padding = np.zeros((data.shape[0], length - data.shape[1]))\n",
        "            data = np.hstack((data, padding))\n",
        "        else:\n",
        "            # Truncate if longer\n",
        "            data = data[:, :length]\n",
        "        return data\n",
        "\n",
        "    def butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        return butter(order, [low, high], btype='band')\n",
        "\n",
        "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n",
        "        b, a = self.butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        return lfilter(b, a, data, axis=1)  # Apply along the sample axis\n",
        "\n",
        "    def calculate_differential_signals(self, data):\n",
        "        num_pairs = len(self.cfg.map_features)\n",
        "        differential_data = np.zeros((num_pairs, data.shape[1]))\n",
        "        for i, (feat_a, feat_b) in enumerate(self.cfg.map_features):\n",
        "            if feat_a in self.feature_to_index and feat_b in self.feature_to_index:\n",
        "                differential_data[i, :] = data[self.feature_to_index[feat_a], :] - data[self.feature_to_index[feat_b], :]\n",
        "            else:\n",
        "                print(f\"Feature {feat_a} or {feat_b} not found in feature_to_index\")\n",
        "        return np.vstack((data, differential_data))\n",
        "\n",
        "    def denoise_filter(self, x):\n",
        "        y = self.butter_bandpass_filter(x, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate, order=6)\n",
        "        y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
        "        y = y[:, 0:-1:4]\n",
        "        return y\n",
        "\n",
        "    def normalize(self, data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        return (data - mean) / (std + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "    def select_and_map_channels(self, data, channels, feature_to_index):\n",
        "        selected_indices = [feature_to_index[ch] for ch in channels if ch in feature_to_index]\n",
        "        differential_indices = list(range(self.differential_channels_start_index, self.differential_channels_start_index + len(self.cfg.map_features)))\n",
        "        selected_data = data[selected_indices + differential_indices, :]\n",
        "        return selected_data\n",
        "\n",
        "    def mirror_eeg(self, data):\n",
        "        indx1 = [self.feature_to_index[x] for x in self.cfg.LL + self.cfg.LP if x in self.feature_to_index]\n",
        "        indx2 = [self.feature_to_index[x] for x in self.cfg.RL + self.cfg.RP if x in self.feature_to_index]\n",
        "        data[indx1, :], data[indx2, :] = data[indx2, :], data[indx1, :]\n",
        "        return data\n",
        "\n",
        "    def labels_to_probabilities(self, labels, num_classes):\n",
        "        labels = torch.eye(num_classes)[labels]\n",
        "        return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opbb6YkKVtFb"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def detect_class_imbalance(dataset):\n",
        "    label_counts = Counter()\n",
        "\n",
        "    for _, label in dataset:\n",
        "        # Assuming `label` is a tensor with probabilities, we can get the class index with the maximum probability\n",
        "        class_idx = torch.argmax(label).item()\n",
        "        label_counts[class_idx] += 1\n",
        "\n",
        "    return label_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk6qzJbEEyce"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool_type='max', pool_size=(2, 2), dropout_p=0.5):\n",
        "        super(Block, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        if pool_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=pool_size)\n",
        "        elif pool_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=pool_size)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout(p=dropout_p)\n",
        "        self.conv1x1 = nn.Conv2d(in_channels, out_channels, kernel_size=1)  # 1x1 conv to match channels for skip connection\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Match the dimensions of identity to x\n",
        "        if identity.shape != x.shape:\n",
        "            identity = F.interpolate(identity, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
        "            identity = self.conv1x1(identity)\n",
        "\n",
        "        x += identity  # Add skip connection\n",
        "        return x\n",
        "\n",
        "class Spectrogram_Model(nn.Module):\n",
        "    def __init__(self, num_classes=6):\n",
        "        super(Spectrogram_Model, self).__init__()\n",
        "        self.block1 = Block(in_channels=3, out_channels=16, pool_type='max', pool_size=(2, 2))\n",
        "        self.block2 = Block(in_channels=16, out_channels=32, pool_type='avg', pool_size=(2, 2))\n",
        "        self.block3 = Block(in_channels=32, out_channels=64, pool_type='max', pool_size=(2, 2))\n",
        "        self.block4 = Block(in_channels=64, out_channels=128, pool_type='avg', pool_size=(2, 2))\n",
        "        self.block5 = Block(in_channels=128, out_channels=256, pool_type='max', pool_size=(2, 2))\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)  # Output: (16, H/2, W/2)\n",
        "        x = self.block2(x)  # Output: (32, H/4, W/4)\n",
        "        x = self.block3(x)  # Output: (64, H/8, W/8)\n",
        "        x = self.block4(x)  # Output: (128, H/16, W/16)\n",
        "        x = self.block5(x)  # Output: (256, H/32, W/32)\n",
        "\n",
        "        x = self.gap(x)  # Global Average Pooling to (256, 1, 1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (256)\n",
        "        x = self.fc(x)  # Fully connected layer\n",
        "        x = self.log_softmax(x)  # Apply LogSoftmax\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7fUY_e1Eyce"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, input_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.query = nn.Linear(input_dim, attention_dim)\n",
        "        self.key = nn.Linear(input_dim, attention_dim)\n",
        "        self.value = nn.Linear(input_dim, attention_dim)\n",
        "        self.scale = attention_dim ** -0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "class EEGNetAttentionRegularized(nn.Module):\n",
        "    def __init__(self, nb_classes, Chans=37, Samples=3000,\n",
        "                 dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25, dropoutType='Dropout'):\n",
        "        super(EEGNetAttentionRegularized, self).__init__()\n",
        "        self.nb_classes = nb_classes\n",
        "        self.Chans = Chans\n",
        "        self.Samples = Samples\n",
        "\n",
        "        # Block 1\n",
        "        self.conv1 = nn.Conv2d(1, F1, (1, kernLength), padding='same', bias=False)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(F1)\n",
        "        self.depthwiseConv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(F1 * D)\n",
        "        self.activation = nn.ELU()\n",
        "        self.avg_pool1 = nn.AvgPool2d((1, 4))\n",
        "        self.dropout1 = nn.Dropout(dropoutRate) if dropoutType == 'Dropout' else nn.Dropout2d(dropoutRate)\n",
        "\n",
        "        # Block 2 using depthwise separable convolution\n",
        "        self.separableConv = nn.Conv2d(F1 * D, F2, (1, 16), padding='same', bias=False)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(F2)\n",
        "        self.avg_pool2 = nn.AvgPool2d((1, 8))\n",
        "        self.dropout2 = nn.Dropout(dropoutRate) if dropoutType == 'Dropout' else nn.Dropout2d(dropoutRate)\n",
        "\n",
        "        # Attention Mechanism\n",
        "        self.attention_layer = Attention(F2, F2)\n",
        "\n",
        "        # Calculate the output size after conv and pooling layers\n",
        "        self.output_samples = self._get_output_size()\n",
        "        self.flattened_size = F2 * self.output_samples\n",
        "\n",
        "        # Classification layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense1 = nn.Linear(self.flattened_size, 128)\n",
        "        self.dropout3 = nn.Dropout(dropoutRate)\n",
        "        self.dense2 = nn.Linear(128, nb_classes)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "        # Add L2 regularization (weight decay)\n",
        "        self.weight_decay = 1e-3  # Example value, tune as necessary\n",
        "\n",
        "    def _get_output_size(self):\n",
        "        # Forward pass of dummy input to calculate output size\n",
        "        with torch.no_grad():\n",
        "            x = torch.zeros(1, 1, self.Chans, self.Samples)\n",
        "            x = self.conv1(x)\n",
        "            x = self.batchnorm1(x)\n",
        "            x = self.depthwiseConv(x)\n",
        "            x = self.batchnorm2(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.avg_pool1(x)\n",
        "            x = self.dropout1(x)\n",
        "            x = self.separableConv(x)\n",
        "            x = self.batchnorm3(x)\n",
        "            x = self.activation(x)\n",
        "            x = self.avg_pool2(x)\n",
        "            x = self.dropout2(x)\n",
        "            b, c, h, w = x.size()\n",
        "            return h * w\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.depthwiseConv(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.avg_pool1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = self.separableConv(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.avg_pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Attention mechanism\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(b, c, h * w)  # Reshape to (batch_size, feature_dim, sequence_length)\n",
        "        x, _ = self.attention_layer(x.permute(0, 2, 1))  # Apply attention and permute back\n",
        "        x = x.permute(0, 2, 1).contiguous()  # Permute back to original shape\n",
        "        x = x.view(b, c, h, w)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense2(x)\n",
        "        x = self.log_softmax(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DW8uYOVEyce"
      },
      "outputs": [],
      "source": [
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, eeg_model, spectrogram_model, num_classes=6):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "        self.eeg_model = eeg_model\n",
        "        self.spectrogram_model = spectrogram_model\n",
        "\n",
        "        # Combining the outputs of the two models\n",
        "        combined_output_size = eeg_model.dense.out_features + spectrogram_model.fc.out_features\n",
        "\n",
        "        self.fc1 = nn.Linear(combined_output_size, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, eeg_data, spectrogram_data):\n",
        "        eeg_output = self.eeg_model(eeg_data)\n",
        "        spectrogram_output = self.spectrogram_model(spectrogram_data)\n",
        "\n",
        "        combined = torch.cat((eeg_output, spectrogram_output), dim=1)\n",
        "\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = self.fc2(x)\n",
        "        x = self.log_softmax(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_spectrogram(self, spectrogram_data):\n",
        "        return self.spectrogram_model(spectrogram_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKn_oXWhEyce"
      },
      "outputs": [],
      "source": [
        "class CombinedDataset(Dataset):\n",
        "    def __init__(self, metadata, cfg=CFG, training_flag=False, augmentations=None, plot=False):\n",
        "        self.metadata = metadata\n",
        "        self.cfg = cfg\n",
        "        self.training_flag = training_flag\n",
        "        self.augmentations = augmentations\n",
        "        self.plot = plot\n",
        "\n",
        "        # Set the random seed for reproducibility\n",
        "        self.cfg.seed_everything(cfg)\n",
        "\n",
        "        # Feature to index mapping from CFG\n",
        "        self.feature_to_index = self.cfg.feature_to_index\n",
        "        self.differential_channels_start_index = len(self.feature_to_index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata.iloc[idx]\n",
        "\n",
        "        # Process EEG data\n",
        "        eeg_data = self.process_eeg(row)\n",
        "        eeg_label = self.get_label(row)\n",
        "\n",
        "        # Process Spectrogram data\n",
        "        spectrogram_data = self.process_spectrogram(row)\n",
        "        spectrogram_label = self.get_label(row)\n",
        "\n",
        "        # Ensure the labels are the same\n",
        "        assert torch.equal(eeg_label, spectrogram_label), \"Labels do not match!\"\n",
        "\n",
        "        return (eeg_data, spectrogram_data), eeg_label\n",
        "\n",
        "    def process_eeg(self, row):\n",
        "        eeg_id = row['eeg_id']\n",
        "        eeg = load_train_eeg_frame(eeg_id)\n",
        "        waves = eeg.values.T\n",
        "\n",
        "        if self.cfg.AUGMENT:\n",
        "            waves = self.mirror_eeg(waves)\n",
        "\n",
        "        waves = self.butter_bandpass_filter(waves, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate)\n",
        "        waves = self.handle_nan(waves)\n",
        "        waves = self.calculate_differential_signals(waves)\n",
        "        waves = self.denoise_filter(waves)\n",
        "        waves = self.normalize(waves)\n",
        "        waves = self.select_and_map_channels(waves, self.cfg.eeg_features, self.feature_to_index)\n",
        "        waves = self.pad_or_truncate(waves, self.cfg.fixed_length)\n",
        "        waves = waves[np.newaxis, ...]  # Add channel dimension for EEG data\n",
        "        return torch.tensor(waves, dtype=torch.float32)\n",
        "\n",
        "    def process_spectrogram(self, row):\n",
        "        spec_id = row['spectrogram_id']\n",
        "        raw_spectrogram = load_train_spectr_frame(spec_id)\n",
        "\n",
        "        if isinstance(raw_spectrogram, pd.DataFrame):\n",
        "            raw_spectrogram = raw_spectrogram.to_numpy()\n",
        "\n",
        "        offset = row.get(\"spectrogram_label_offset_seconds\", None)\n",
        "        if offset is not None:\n",
        "            offset = offset // 2\n",
        "            basic_spectrogram = raw_spectrogram[:, offset:offset + 300]\n",
        "            pad_size = max(0, 300 - basic_spectrogram.shape[1])\n",
        "            basic_spectrogram = np.pad(basic_spectrogram, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        else:\n",
        "            basic_spectrogram = raw_spectrogram\n",
        "\n",
        "        spectrogram = basic_spectrogram.T\n",
        "        spectrogram = self.pad_or_truncate(spectrogram, self.cfg.image_size)\n",
        "        spectrogram = self.handle_nan(spectrogram)\n",
        "        spectrogram = self.baseline_correction(spectrogram)\n",
        "        spectrogram = self.apply_notch_filter(spectrogram)\n",
        "        spectrogram = self.smooth_spectrogram(spectrogram)\n",
        "        spectrogram = self.normalize_signal(spectrogram)\n",
        "        spectrogram = self.resample_spectrogram(spectrogram, self.cfg.image_size)\n",
        "        spectrogram = np.tile(spectrogram[..., None], (1, 1, 3))\n",
        "\n",
        "        if self.plot:\n",
        "            self.plot_spectrograms(basic_spectrogram, spectrogram, self.metadata.index.tolist(), num_labels=10)\n",
        "\n",
        "        if self.augmentations:\n",
        "            spectrogram = (spectrogram * 255).astype(np.uint8)\n",
        "            augmented = self.augmentations(image=spectrogram)\n",
        "            spectrogram = augmented['image']\n",
        "            spectrogram = spectrogram.float() / 255.0\n",
        "        else:\n",
        "            spectrogram = spectrogram.astype(np.float32)\n",
        "            spectrogram = torch.tensor(spectrogram).permute(2, 0, 1).float()\n",
        "\n",
        "        return spectrogram\n",
        "\n",
        "    def get_label(self, row):\n",
        "        label_name = row['expert_consensus']\n",
        "        label_idx = self.cfg.name2label[label_name]\n",
        "        label = labels_to_probabilities(label_idx, self.cfg.n_classes)\n",
        "        return label.clone().detach().float()\n",
        "\n",
        "    def handle_nan(self, data):\n",
        "        \"\"\"Handle NaN values by replacing them with the mean of the respective channels.\"\"\"\n",
        "        data = data[~np.isnan(data).all(axis=1)]\n",
        "        if data.size == 0:\n",
        "            data = np.zeros((self.cfg.in_channels + len(self.cfg.map_features), self.cfg.fixed_length))\n",
        "        else:\n",
        "            where_nan = np.isnan(data)\n",
        "            mean_values = np.nanmean(data, axis=1, keepdims=True)\n",
        "            mean_values[np.isnan(mean_values)] = 0\n",
        "            data[where_nan] = np.take(mean_values, np.where(where_nan)[0])\n",
        "        return data\n",
        "\n",
        "    def pad_or_truncate(self, data, length):\n",
        "        if isinstance(length, int):\n",
        "            if data.shape[1] < length:\n",
        "                padding = np.zeros((data.shape[0], length - data.shape[1]))\n",
        "                data = np.hstack((data, padding))\n",
        "            else:\n",
        "                data = data[:, :length]\n",
        "        elif isinstance(length, tuple):\n",
        "            target_rows, target_cols = length\n",
        "            if data.shape[0] < target_rows:\n",
        "                row_padding = np.zeros((target_rows - data.shape[0], data.shape[1]))\n",
        "                data = np.vstack((data, row_padding))\n",
        "            else:\n",
        "                data = data[:target_rows, :]\n",
        "            if data.shape[1] < target_cols:\n",
        "                col_padding = np.zeros((data.shape[0], target_cols - data.shape[1]))\n",
        "                data = np.hstack((data, col_padding))\n",
        "            else:\n",
        "                data = data[:, :target_cols]\n",
        "        return data\n",
        "\n",
        "    def butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        return butter(order, [low, high], btype='band')\n",
        "\n",
        "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n",
        "        b, a = self.butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        return lfilter(b, a, data, axis=1)\n",
        "\n",
        "    def calculate_differential_signals(self, data):\n",
        "        num_pairs = len(self.cfg.map_features)\n",
        "        differential_data = np.zeros((num_pairs, data.shape[1]))\n",
        "        for i, (feat_a, feat_b) in enumerate(self.cfg.map_features):\n",
        "            if feat_a in self.feature_to_index and feat_b in self.feature_to_index:\n",
        "                differential_data[i, :] = data[self.feature_to_index[feat_a], :] - data[self.feature_to_index[feat_b], :]\n",
        "            else:\n",
        "                print(f\"Feature {feat_a} or {feat_b} not found in feature_to_index\")\n",
        "        return np.vstack((data, differential_data))\n",
        "\n",
        "    def denoise_filter(self, x):\n",
        "        y = self.butter_bandpass_filter(x, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate, order=6)\n",
        "        y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
        "        y = y[:, 0:-1:4]\n",
        "        return y\n",
        "\n",
        "    def normalize(self, data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        return (data - mean) / (std + 1e-6)\n",
        "\n",
        "    def select_and_map_channels(self, data, channels, feature_to_index):\n",
        "        selected_indices = [feature_to_index[ch] for ch in channels if ch in feature_to_index]\n",
        "        differential_indices = list(range(self.differential_channels_start_index, self.differential_channels_start_index + len(self.cfg.map_features)))\n",
        "        selected_data = data[selected_indices + differential_indices, :]\n",
        "        return selected_data\n",
        "\n",
        "    def mirror_eeg(self, data):\n",
        "        indx1 = [self.cfg.feature_to_index[x] for x in self.cfg.LL + self.cfg.LP if x in self.cfg.feature_to_index]\n",
        "        indx2 = [self.cfg.feature_to_index[x] for x in self.cfg.RL + self.cfg.RP if x in self.cfg.feature_to_index]\n",
        "        data[indx1, :], data[indx2, :] = data[indx2, :], data[indx1, :]\n",
        "        return data\n",
        "\n",
        "    def baseline_correction(self, sig):\n",
        "        sig -= np.mean(sig, axis=0)\n",
        "        return sig\n",
        "\n",
        "    def normalize_signal(self, sig):\n",
        "        sig = np.nan_to_num(sig, nan=np.nanmean(sig))\n",
        "        return (sig - np.min(sig)) / (np.max(sig) - np.min(sig) + 1e-6)\n",
        "\n",
        "    def apply_notch_filter(self, sig, freq=60, fs=200, quality=30):\n",
        "        b, a = iirnotch(freq, quality, fs)\n",
        "        sig = filtfilt(b, a, sig, axis=0)\n",
        "        return sig\n",
        "\n",
        "    def smooth_spectrogram(self, sig, sigma=1.0):\n",
        "        sig = gaussian_filter(sig, sigma=sigma)\n",
        "        return sig\n",
        "\n",
        "    def resample_spectrogram(self, sig, target_shape):\n",
        "        sig = resize(sig, target_shape, mode='reflect', anti_aliasing=True)\n",
        "        return sig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWyT_gkKYJLt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdNU0i-LEycf"
      },
      "outputs": [],
      "source": [
        "def train_and_validate_eeg(model, train_loader, valid_loader, epochs, optimizer, criterion, scheduler, device, checkpoint_dir):\n",
        "    checkpoint_filename = \"eeg_checkpoint.pth.tar\"\n",
        "\n",
        "    start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies, lr_scheduler,regularization_losses = load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        running_reg_loss = 0.0  # Initialize running regularization loss\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Manually compute regularization loss\n",
        "            reg_loss = 0\n",
        "            for param in model.parameters():\n",
        "                reg_loss += torch.sum(param ** 2)\n",
        "            reg_loss *= model.weight_decay  # Use weight_decay from the optimizer\n",
        "\n",
        "            total_loss = loss + reg_loss  # Combine regularization loss with main loss\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            running_reg_loss += reg_loss.item()  # Accumulate the regularization loss\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            _, labels_max = torch.max(labels, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels_max).sum().item()\n",
        "\n",
        "        train_loss = running_train_loss / total_train\n",
        "        reg_loss_avg = running_reg_loss / total_train  # Average regularization loss\n",
        "        train_acc = 100. * correct_train / total_train\n",
        "        train_losses.append(train_loss)\n",
        "        regularization_losses.append(reg_loss_avg)  # Track the regularization loss\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        running_valid_loss = 0.0\n",
        "        correct_valid = 0\n",
        "        total_valid = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, labels in valid_loader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_valid_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                _, labels_max = torch.max(labels, 1)\n",
        "                total_valid += labels.size(0)\n",
        "                correct_valid += (predicted == labels_max).sum().item()\n",
        "\n",
        "        valid_loss = running_valid_loss / total_valid\n",
        "        valid_acc = 100. * correct_valid / total_valid\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Reg Loss: {reg_loss_avg:.4f}, Train Accuracy: {train_acc:.2f}%\"\n",
        "              f\" - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.2f}%\")\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(valid_loss)\n",
        "            print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "            lr_scheduler.append(scheduler.get_last_lr()[0])\n",
        "\n",
        "        if CFG.checkpointing_enabled:\n",
        "            state = {\n",
        "                'epoch': epoch + 1,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'valid_losses': valid_losses,\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'valid_accuracies': valid_accuracies,\n",
        "                'lr_scheduler': lr_scheduler,\n",
        "                'regularization_losses': regularization_losses  # Save the regularization losses\n",
        "            }\n",
        "            save_checkpoint(state, checkpoint_dir, checkpoint_filename)\n",
        "\n",
        "    # Plot the learning rate history\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(lr_scheduler)\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(regularization_losses)\n",
        "    plt.title('Regularization Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Regularization Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6k5L0qtEycf"
      },
      "outputs": [],
      "source": [
        "def compute_regularization_loss(model, l2_lambda):\n",
        "    l2_reg = torch.tensor(0.).to(CFG.device)\n",
        "    for param in model.parameters():\n",
        "        l2_reg += torch.norm(param)\n",
        "    return l2_lambda * l2_reg\n",
        "\n",
        "def train_and_validate_spec(model, train_loader, valid_loader, epochs, optimizer, criterion,scheduler, device,checkpoint_dir,l2_lambda=0.01,):\n",
        "    checkpoint_filename = \"spec_checkpoint.pth.tar\"\n",
        "\n",
        "    start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies = load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            for data, labels in tepoch:\n",
        "                tepoch.set_description(f\"Training Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, labels)\n",
        "                reg_loss = compute_regularization_loss(model, l2_lambda)\n",
        "                total_loss = loss + reg_loss\n",
        "\n",
        "\n",
        "                total_loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += total_loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                _, labels_max = torch.max(labels, 1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += (predicted == labels_max).sum().item()\n",
        "\n",
        "                tepoch.set_postfix(loss=running_train_loss / (len(train_loader) * train_loader.batch_size), accuracy=100. * (correct_train / total_train))\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "        train_accuracy = 100. * (correct_train / total_train)\n",
        "\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        running_valid_loss = 0.0\n",
        "        correct_valid = 0\n",
        "        total_valid = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(valid_loader, unit=\"batch\") as vepoch:\n",
        "                for data, labels in vepoch:\n",
        "                    vepoch.set_description(f\"Validation Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "                    data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = model(data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    reg_loss = compute_regularization_loss(model, l2_lambda)\n",
        "                    total_loss = loss + reg_loss\n",
        "                    running_valid_loss += total_loss.item()\n",
        "\n",
        "                    _, predicted = torch.max(outputs, 1)\n",
        "                    _, labels_max = torch.max(labels, 1)\n",
        "                    total_valid += labels.size(0)\n",
        "                    correct_valid += (predicted == labels_max).sum().item()\n",
        "\n",
        "                    vepoch.set_postfix(loss=running_valid_loss / (len(valid_loader) * valid_loader.batch_size), accuracy=100. * (correct_valid / total_valid))\n",
        "\n",
        "        valid_loss = running_valid_loss / len(valid_loader)\n",
        "        valid_accuracy = 100. * correct_valid / total_valid\n",
        "\n",
        "\n",
        "\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}%\")\n",
        "        print(f\"            - Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step(valid_loss)\n",
        "\n",
        "\n",
        "        state = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'valid_losses': valid_losses,\n",
        "            'train_accuracies': train_accuracies,\n",
        "            'valid_accuracies': valid_accuracies,\n",
        "            'lr_scheduler': scheduler.state_dict() if scheduler else None\n",
        "        }\n",
        "\n",
        "        if CFG.checkpointing_enabled:\n",
        "            save_checkpoint(state, checkpoint_dir, checkpoint_filename)\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC1ZhlVeEycf"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram_only(processed, num_labels=10, save_path='processed_spectrogram.png'):\n",
        "    # Convert tensor to numpy array\n",
        "    if isinstance(processed, torch.Tensor):\n",
        "        processed = processed.cpu().numpy()\n",
        "\n",
        "    x_ticks = np.linspace(0, processed.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = x_ticks\n",
        "\n",
        "    plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.title(\"Processed Spectrogram\")\n",
        "    if processed.ndim == 3 and processed.shape[2] > 1:\n",
        "        plt.imshow(processed[:, :, 0], aspect='auto', cmap='viridis')\n",
        "    else:\n",
        "        plt.imshow(processed.squeeze(), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)  # Save the plot to a file\n",
        "    plt.show()\n",
        "\n",
        "# Define the prediction function for LIME\n",
        "def predict_fn(images, model, device):\n",
        "    images = [transforms.ToTensor()(Image.fromarray(img.astype(np.uint8))).unsqueeze(0).to(device) for img in images]\n",
        "    images = torch.cat(images, dim=0)\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "    probs = softmax(outputs, dim=1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "# Initialize the LIME image explainer\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "def train_and_validate_combined(model, train_loader, valid_loader, epochs, optimizer, criterion, device, checkpoint_dir, n=2, sample_spectrogram=None):\n",
        "    checkpoint_filename = \"combined_checkpoint.pth.tar\"\n",
        "\n",
        "    start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies = load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        running_corrects_train = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "            for batch in tepoch:\n",
        "                tepoch.set_description(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "                (eeg_data, spectrogram_data), labels = batch\n",
        "                eeg_data, spectrogram_data, labels = eeg_data.to(device), spectrogram_data.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(eeg_data, spectrogram_data)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_train_loss += loss.item() * eeg_data.size(0)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                labels = torch.argmax(labels, dim=1)\n",
        "                running_corrects_train += torch.sum(preds == labels).item()\n",
        "                total_train_samples += eeg_data.size(0)\n",
        "\n",
        "        epoch_train_loss = running_train_loss / total_train_samples\n",
        "        epoch_train_acc = running_corrects_train / total_train_samples * 100\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_acc)\n",
        "\n",
        "        model.eval()\n",
        "        running_valid_loss = 0.0\n",
        "        running_corrects_valid = 0\n",
        "        total_valid_samples = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(valid_loader, unit=\"batch\") as vepoch:\n",
        "                for batch in vepoch:\n",
        "                    vepoch.set_description(f\"Validating Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "                    (eeg_data, spectrogram_data), labels = batch\n",
        "                    eeg_data, spectrogram_data, labels = eeg_data.to(device), spectrogram_data.to(device), labels.to(device)\n",
        "                    outputs = model(eeg_data, spectrogram_data)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    running_valid_loss += loss.item() * eeg_data.size(0)\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                    labels = torch.argmax(labels, dim=1)\n",
        "                    running_corrects_valid += torch.sum(preds == labels).item()\n",
        "                    total_valid_samples += eeg_data.size(0)\n",
        "\n",
        "        epoch_valid_loss = running_valid_loss / total_valid_samples\n",
        "        epoch_valid_acc = running_corrects_valid / total_valid_samples * 100\n",
        "\n",
        "        valid_losses.append(epoch_valid_loss)\n",
        "        valid_accuracies.append(epoch_valid_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}:\")\n",
        "        print(f\"  Train Loss: {epoch_train_loss:.4f} Train Accuracy: {epoch_train_acc:.2f}%\")\n",
        "        print(f\"  Valid Loss: {epoch_valid_loss:.4f} Valid Accuracy: {epoch_valid_acc:.2f}%\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'valid_losses': valid_losses,\n",
        "            'train_accuracies': train_accuracies,\n",
        "            'valid_accuracies': valid_accuracies\n",
        "        }, checkpoint_dir, checkpoint_filename)\n",
        "\n",
        "        # Perform LIME analysis every n epochs\n",
        "        if (epoch + 1) % n == 0 and sample_spectrogram is not None:\n",
        "            model.eval()\n",
        "            sample_spectrogram = sample_spectrogram.cpu().numpy()\n",
        "\n",
        "\n",
        "            # Apply SLIC algorithm to segment the image\n",
        "            segments = slic(sample_spectrogram_np, n_segments=100, compactness=10, sigma=1)\n",
        "\n",
        "            # Explain the image using LIME\n",
        "            explanation = explainer.explain_instance(sample_spectrogram_np, lambda x: predict_fn(x, model.spectrogram_model, device), top_labels=1, hide_color=0, num_samples=100, segmentation_fn=lambda x: slic(x, n_segments=100, compactness=10, sigma=1))\n",
        "\n",
        "            # Get the explanation for the top label\n",
        "            temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=False)\n",
        "\n",
        "            # Overlay the mask on the original image\n",
        "            img_boundry = mark_boundaries(temp / 255.0, mask)\n",
        "\n",
        "            # Plot and save the LIME explanation\n",
        "            plt.imshow(img_boundry)\n",
        "            plt.title(f'LIME Explanation Epoch {epoch + 1}')\n",
        "            plt.savefig(f'LIME_Epoch_{epoch + 1}.png')\n",
        "            plt.close()\n",
        "\n",
        "    return train_losses, valid_losses, train_accuracies, valid_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHMZ_hW6B_jq"
      },
      "outputs": [],
      "source": [
        "fold_indices = create_k_fold_splits(main_metadata, n_splits=5)\n",
        "for fold_idx in range(len(fold_indices)):\n",
        "    train_metadata, valid_metadata = createTrainTestSplit(main_metadata, fold_indices, fold_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WoXKrvs-bHyY",
        "outputId": "c53a0efc-a8f3-4cb1-8c6d-acfcdc827a39"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-f570327bf58f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meeg_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHMS_EEG_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meeg_label_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_class_imbalance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg_train_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"EEG Dataset Class Distribution: {eeg_label_counts}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspec_train_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHMS_Spectrogram_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-567068f961e4>\u001b[0m in \u001b[0;36mdetect_class_imbalance\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabel_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Assuming `label` is a tensor with probabilities, we can get the class index with the maximum probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-ea0111a0c61e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_map_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mlabel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expert_consensus'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming 'expert_consensus' column contains the label names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mlabel_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname2label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-ea0111a0c61e>\u001b[0m in \u001b[0;36msingle_map_func\u001b[0;34m(self, row, is_training)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m# Skip this sample if data is empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_differential_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenoise_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-ea0111a0c61e>\u001b[0m in \u001b[0;36mcalculate_differential_signals\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Feature {feat_a} or {feat_b} not found in feature_to_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdifferential_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdenoise_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "eeg_train_dataset = HMS_EEG_Dataset(train_metadata, cfg=CFG)\n",
        "eeg_label_counts = detect_class_imbalance(eeg_train_dataset)\n",
        "print(f\"EEG Dataset Class Distribution: {eeg_label_counts}\")\n",
        "\n",
        "spec_train_dataset = HMS_Spectrogram_Dataset(train_metadata, cfg=CFG)\n",
        "spec_label_counts = detect_class_imbalance(spec_train_dataset)\n",
        "print(f\"Spectrogram Dataset Class Distribution: {spec_label_counts}\")\n",
        "\n",
        "combined_dataset = CombinedDataset(metadata=train_metadata, cfg=CFG)\n",
        "combined_label_counts = detect_class_imbalance(combined_dataset)\n",
        "print(f\"Combined Dataset Class Distribution: {combined_label_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXh2Rw8Jbqbk"
      },
      "outputs": [],
      "source": [
        "def plot_class_distribution(label_counts, title=\"Class Distribution\"):\n",
        "    classes = list(label_counts.keys())\n",
        "    counts = list(label_counts.values())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(classes, counts, color='skyblue')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.title(title)\n",
        "    plt.xticks(classes)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_class_distribution(eeg_label_counts, title=\"EEG Dataset Class Distribution\")\n",
        "plot_class_distribution(spec_label_counts, title=\"Spectrogram Dataset Class Distribution\")\n",
        "plot_class_distribution(combined_label_counts, title=\"Combined Dataset Class Distribution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exKMh6AJEycf",
        "outputId": "e833c4d1-aca7-4c11-a7b2-ef4750e40b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the train dataset: 45945\n",
            "Size of the valid dataset: 21580\n",
            "Batch data shape: torch.Size([1024, 1, 37, 3000])\n",
            "Batch labels shape: torch.Size([1024, 6])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "eeg_train_dataset = HMS_EEG_Dataset(train_metadata, cfg=CFG)\n",
        "print(f\"Size of the train dataset: {len(eeg_train_dataset)}\")\n",
        "\n",
        "eeg_valid_dataset = HMS_EEG_Dataset(valid_metadata, cfg=CFG)\n",
        "print(f\"Size of the valid dataset: {len(eeg_valid_dataset)}\")\n",
        "\n",
        "\n",
        "eeg_train_loader = DataLoader(eeg_train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "eeg_valid_loader = DataLoader(eeg_valid_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "\n",
        "    # Iterate over batches and print the shape of the final DataLoader object\n",
        "for batch in eeg_train_loader:\n",
        "    data, labels = batch\n",
        "    print(f\"Batch data shape: {data.shape}\")  # Should print (batch_size, 1, channels, samples)\n",
        "    print(f\"Batch labels shape: {labels.shape}\")  # Should print (batch_size,)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2vFwHx9tmY0",
        "outputId": "86c027a0-71bd-4ebb-f2f5-42ace054a19d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EEGNetAttentionRegularized(\n",
            "  (conv1): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=same, bias=False)\n",
            "  (batchnorm1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (depthwiseConv): Conv2d(8, 16, kernel_size=(37, 1), stride=(1, 1), groups=8, bias=False)\n",
            "  (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (activation): ELU(alpha=1.0)\n",
            "  (avg_pool1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (separableConv): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=same, bias=False)\n",
            "  (batchnorm3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (avg_pool2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (attention_layer): Attention(\n",
            "    (query): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (key): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (value): Linear(in_features=16, out_features=16, bias=True)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (dense1): Linear(in_features=1488, out_features=128, bias=True)\n",
            "  (dropout3): Dropout(p=0.5, inplace=False)\n",
            "  (dense2): Linear(in_features=128, out_features=6, bias=True)\n",
            "  (log_softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 8, 37, 3000]             512\n",
            "       BatchNorm2d-2          [-1, 8, 37, 3000]              16\n",
            "            Conv2d-3          [-1, 16, 1, 3000]             592\n",
            "       BatchNorm2d-4          [-1, 16, 1, 3000]              32\n",
            "               ELU-5          [-1, 16, 1, 3000]               0\n",
            "         AvgPool2d-6           [-1, 16, 1, 750]               0\n",
            "           Dropout-7           [-1, 16, 1, 750]               0\n",
            "            Conv2d-8           [-1, 16, 1, 750]           4,096\n",
            "       BatchNorm2d-9           [-1, 16, 1, 750]              32\n",
            "              ELU-10           [-1, 16, 1, 750]               0\n",
            "        AvgPool2d-11            [-1, 16, 1, 93]               0\n",
            "          Dropout-12            [-1, 16, 1, 93]               0\n",
            "           Linear-13               [-1, 93, 16]             272\n",
            "           Linear-14               [-1, 93, 16]             272\n",
            "           Linear-15               [-1, 93, 16]             272\n",
            "        Attention-16  [[-1, 93, 16], [-1, 93, 93]]               0\n",
            "          Flatten-17                 [-1, 1488]               0\n",
            "           Linear-18                  [-1, 128]         190,592\n",
            "          Dropout-19                  [-1, 128]               0\n",
            "           Linear-20                    [-1, 6]             774\n",
            "       LogSoftmax-21                    [-1, 6]               0\n",
            "================================================================\n",
            "Total params: 197,462\n",
            "Trainable params: 197,462\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.42\n",
            "Forward/backward pass size (MB): 83.01\n",
            "Params size (MB): 0.75\n",
            "Estimated Total Size (MB): 84.19\n",
            "----------------------------------------------------------------\n",
            "Training EEG Model.\n"
          ]
        }
      ],
      "source": [
        "       # Instantiate the model\n",
        "EEG_model = EEGNetAttentionRegularized(nb_classes=6, Chans=37, Samples=3000)\n",
        "EEG_model.to(CFG.device)\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')  # KLDivLoss for classification\n",
        "# Define the optimizer, including the weight_decay parameter\n",
        "optimizer = torch.optim.Adam(EEG_model.parameters(), lr=0.003, weight_decay=EEG_model.weight_decay)\n",
        "# Initialize the scheduler with desired parameters\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.995,     # Very small reduction each time\n",
        "    patience=2,       # Frequent reductions (every 2 epochs of no improvement)\n",
        "    threshold=0.0001, # Improvement threshold\n",
        "    threshold_mode='rel',\n",
        "    cooldown=0,       # No cooldown period\n",
        "    min_lr=1e-6       # Ensure learning rate never reduces to zero\n",
        ")\n",
        "print(EEG_model)\n",
        "\n",
        "# Print the summary of the model\n",
        "summary(EEG_model, input_size=(1, 37, 3000))\n",
        "print(\"Training EEG Model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRWJq907tU_B",
        "outputId": "b8f1c5e4-5c12-4fc7-ca50-45a7c110c02f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint found at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 1/50 - Train Loss: 0.0013, Reg Loss: 0.0001, Train Accuracy: 49.13% - Valid Loss: 0.0009, Valid Accuracy: 67.04%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 2/50 - Train Loss: 0.0010, Reg Loss: 0.0001, Train Accuracy: 64.66% - Valid Loss: 0.0008, Valid Accuracy: 71.47%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 3/50 - Train Loss: 0.0009, Reg Loss: 0.0001, Train Accuracy: 68.64% - Valid Loss: 0.0008, Valid Accuracy: 73.78%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 4/50 - Train Loss: 0.0008, Reg Loss: 0.0001, Train Accuracy: 71.06% - Valid Loss: 0.0006, Valid Accuracy: 79.84%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 5/50 - Train Loss: 0.0008, Reg Loss: 0.0001, Train Accuracy: 71.91% - Valid Loss: 0.0006, Valid Accuracy: 78.73%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 6/50 - Train Loss: 0.0008, Reg Loss: 0.0001, Train Accuracy: 73.20% - Valid Loss: 0.0007, Valid Accuracy: 75.38%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 7/50 - Train Loss: 0.0008, Reg Loss: 0.0001, Train Accuracy: 73.90% - Valid Loss: 0.0006, Valid Accuracy: 80.51%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 8/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 74.40% - Valid Loss: 0.0006, Valid Accuracy: 81.09%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 9/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 74.63% - Valid Loss: 0.0006, Valid Accuracy: 81.19%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 10/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 75.35% - Valid Loss: 0.0006, Valid Accuracy: 81.15%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 11/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 75.70% - Valid Loss: 0.0006, Valid Accuracy: 82.03%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 12/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 76.03% - Valid Loss: 0.0006, Valid Accuracy: 81.79%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 13/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 76.43% - Valid Loss: 0.0006, Valid Accuracy: 81.88%\n",
            "Learning Rate: 0.003000\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 14/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 76.84% - Valid Loss: 0.0006, Valid Accuracy: 81.03%\n",
            "Learning Rate: 0.002985\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 15/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 77.06% - Valid Loss: 0.0006, Valid Accuracy: 81.10%\n",
            "Learning Rate: 0.002985\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 16/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 76.72% - Valid Loss: 0.0006, Valid Accuracy: 80.19%\n",
            "Learning Rate: 0.002985\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 17/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 77.33% - Valid Loss: 0.0007, Valid Accuracy: 76.50%\n",
            "Learning Rate: 0.002970\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 18/50 - Train Loss: 0.0006, Reg Loss: 0.0001, Train Accuracy: 77.81% - Valid Loss: 0.0006, Valid Accuracy: 80.44%\n",
            "Learning Rate: 0.002970\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 19/50 - Train Loss: 0.0007, Reg Loss: 0.0001, Train Accuracy: 77.66% - Valid Loss: 0.0006, Valid Accuracy: 80.78%\n",
            "Learning Rate: 0.002970\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 20/50 - Train Loss: 0.0006, Reg Loss: 0.0001, Train Accuracy: 78.20% - Valid Loss: 0.0007, Valid Accuracy: 78.38%\n",
            "Learning Rate: 0.002955\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 21/50 - Train Loss: 0.0006, Reg Loss: 0.0001, Train Accuracy: 77.81% - Valid Loss: 0.0006, Valid Accuracy: 81.13%\n",
            "Learning Rate: 0.002955\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n",
            "Epoch 22/50 - Train Loss: 0.0006, Reg Loss: 0.0001, Train Accuracy: 77.83% - Valid Loss: 0.0006, Valid Accuracy: 80.93%\n",
            "Learning Rate: 0.002955\n",
            "Checkpoint saved at '/content/checkpoint_dir/eeg_checkpoint.pth.tar'\n"
          ]
        }
      ],
      "source": [
        "train_losses, valid_losses, train_accuracies, valid_accuracies = train_and_validate_eeg(\n",
        "    EEG_model, eeg_train_loader, eeg_valid_loader, epochs=50, optimizer=optimizer, criterion=criterion, scheduler=scheduler, device=CFG.device, checkpoint_dir='/content/checkpoint_dir/'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNH5-lzvO6o-"
      },
      "outputs": [],
      "source": [
        "# Unload the model from GPU memory\n",
        "EEG_model.to('cpu')\n",
        "del EEG_model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Model has been unloaded from GPU memory and deleted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMqUAppWEycf"
      },
      "outputs": [],
      "source": [
        "#CFG.stop_checkpointing()\n",
        "#CFG.debug = True\n",
        "#metadata = main_metadata.copy()\n",
        "#if CFG.AUGMENT:\n",
        "    #train_metadata,valid_metadata,augmentation_metadata = createTrainTestSplit(metadata)\n",
        "    #print(f\"Size of the training dataset: {len(train_metadata)}\")\n",
        "    #print(f\"Size of the validation dataset: {len(valid_metadata)}\")\n",
        "    #print(f\"Size of the augmentation dataset: {len(augmentation_metadata)}\")\n",
        "\n",
        "\n",
        "CFG.start_checkpointing()\n",
        "CFG.debug = True\n",
        "CFG.AUGMENT = False\n",
        "metadata = main_metadata.copy()\n",
        "train_metadata,valid_metadata, = createTrainTestSplit(metadata)\n",
        "print(f\"Size of the training dataset: {len(train_metadata)}\")\n",
        "print(f\"Size of the validation dataset: {len(valid_metadata)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FBcsyOLEycg"
      },
      "outputs": [],
      "source": [
        "ref_dataset = HMS_Spectrogram_Dataset(augmentation_metadata, cfg=CFG)\n",
        "data_loader = DataLoader(ref_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
        "if CFG.SPEC:\n",
        "    spec_train_dataset = HMS_Spectrogram_Dataset(train_metadata, cfg=CFG)\n",
        "    #train_dataset = SpectrogramDataset(train_metadata, cfg=CFG, augmentations = augmentations)\n",
        "    spec_train_loader = DataLoader(spec_train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "\n",
        "    spec_valid_dataset = HMS_Spectrogram_Dataset(valid_metadata, cfg=CFG,)\n",
        "    #valid_dataset = SpectrogramDataset(valid_metadata, cfg=CFG, augmentations=augmentations)\n",
        "    spec_valid_loader = DataLoader(spec_valid_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers)\n",
        "\n",
        "\n",
        "    for batch in spec_train_loader:\n",
        "        data, labels = batch\n",
        "        print(f\"Batch data shape: {data.shape}\")  # Should print (batch_size, 1, channels, samples)\n",
        "        print(f\"Batch labels shape: {labels.shape}\")  # Should print (batch_size,)\n",
        "        break\n",
        "\n",
        "    # Example usage\n",
        "    Spectrogram_model = Spectrogram_Model(num_classes=6)\n",
        "    Spectrogram_model.to(CFG.device)\n",
        "    criterion = nn.KLDivLoss(reduction='batchmean')  # KLDivLoss for classification\n",
        "    # Define the optimizer\n",
        "    optimizer = torch.optim.AdamW(Spectrogram_model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "    # Define L2 regularization value\n",
        "    l2_lambda = 0.01\n",
        "\n",
        "    # Optionally, define a learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
        "    print(Spectrogram_model)\n",
        "\n",
        "\n",
        "    device_str = str(CFG.device)\n",
        "\n",
        "    # Print the model summary\n",
        "    summary(Spectrogram_model, input_size=(3, 400, 300), device=device_str)\n",
        "    train_losses, valid_losses, train_accuracies, valid_accuracies = train_and_validate_spec(Spectrogram_model, spec_train_loader, spec_valid_loader, CFG.EPOCHS, optimizer, criterion,scheduler, CFG.device, checkpoint_dir=CFG.checkpoint_dir,l2_lambda=l2_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_lGxH5zEycg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_tGqGRHEycg"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram_only(processed, labels, num_labels=10, save_path='processed_spectrogram.png'):\n",
        "    x_ticks = np.linspace(0, processed.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = [labels[i] for i in x_ticks]\n",
        "\n",
        "    plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.title(\"Processed Spectrogram\")\n",
        "    if processed.ndim == 3 and processed.shape[2] > 1:\n",
        "        plt.imshow(processed[:, :, 0], aspect='auto', cmap='viridis')\n",
        "    else:\n",
        "        plt.imshow(processed.squeeze(), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)  # Save the plot to a file\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fys4dOhHEycg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Create an instance of CombinedDataset\n",
        "train_dataset = CombinedDataset(metadata=train_metadata, cfg=CFG, training_flag=True, augmentations=None, plot=False)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=True)\n",
        "\n",
        "# Visualize the shape of data in each batch\n",
        "for batch_idx, ((eeg_data, spectrogram_data), labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx+1}:\")\n",
        "    print(f\"  EEG Data Shape: {eeg_data.shape}\")  # Should be (batch_size, 1, 37, 3000)\n",
        "    print(f\"  Spectrogram Data Shape: {spectrogram_data.shape}\")  # Should be (batch_size, 3, 400, 300)\n",
        "    print(f\"  Labels Shape: {labels.shape}\")  # Should be (batch_size, 6)\n",
        "    break  # Remove this break statement to print shapes for all batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arCWFXIhEycg"
      },
      "outputs": [],
      "source": [
        "# Create an instance of CombinedDataset\n",
        "valid_dataset = CombinedDataset(metadata=valid_metadata, cfg=CFG, training_flag=True, augmentations=None, plot=False)\n",
        "\n",
        "# Create a DataLoader\n",
        "valid_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=True)\n",
        "\n",
        "# Visualize the shape of data in each batch\n",
        "for batch_idx, ((eeg_data, spectrogram_data), labels) in enumerate(valid_loader):\n",
        "    print(f\"Batch {batch_idx+1}:\")\n",
        "    print(f\"  EEG Data Shape: {eeg_data.shape}\")  # Should be (batch_size, 1, 37, 3000)\n",
        "    print(f\"  Spectrogram Data Shape: {spectrogram_data.shape}\")  # Should be (batch_size, 3, 400, 300)\n",
        "    print(f\"  Labels Shape: {labels.shape}\")  # Should be (batch_size, 6)\n",
        "    break  # Remove this break statement to print shapes for all batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ6rblZLEycg"
      },
      "outputs": [],
      "source": [
        "# Instantiate the models\n",
        "eeg_model = EEGNet(nb_classes=6)\n",
        "spectrogram_model = Spectrogram_Model(num_classes=6)\n",
        "multimodal_model = MultimodalModel(eeg_model, spectrogram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5nn7xhYEycg"
      },
      "outputs": [],
      "source": [
        "print(multimodal_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-KyEyNIEycg"
      },
      "outputs": [],
      "source": [
        "def summary_multimodal(model, eeg_input_size, spectrogram_input_size, device, output_image_path):\n",
        "    model.to(device)\n",
        "    eeg_data = torch.zeros(1, *eeg_input_size).to(device)\n",
        "    spectrogram_data = torch.zeros(1, *spectrogram_input_size).to(device)\n",
        "\n",
        "    # Capture the model summary as text\n",
        "    import io\n",
        "    from contextlib import redirect_stdout\n",
        "\n",
        "    str_buffer = io.StringIO()\n",
        "    with redirect_stdout(str_buffer):\n",
        "        summary(model, input_size=[eeg_data.shape[1:], spectrogram_data.shape[1:]], device=device.type)\n",
        "    summary_str = str_buffer.getvalue()\n",
        "\n",
        "    # Print input shapes for reference (not included in the screenshot)\n",
        "    print(f\"EEG Input Tensor Shape: {eeg_data.shape}\")\n",
        "    print(f\"Spectrogram Input Tensor Shape: {spectrogram_data.shape}\")\n",
        "\n",
        "    # Render the summary text into an image\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    ax.text(0.1, 0.9, summary_str, fontsize=12, family='monospace', va='top')\n",
        "    ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the figure\n",
        "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0.1)\n",
        "    plt.close(fig)\n",
        "\n",
        "# Define input sizes for summary\n",
        "eeg_input_size = (1, 37, 3000)  # Adding the batch size dimension\n",
        "spectrogram_input_size = (3, 400, 300)  # Already includes the batch size dimension\n",
        "\n",
        "# Get summary of the multimodal model and save as a screenshot\n",
        "output_image_path = \"multimodal_model_summary.png\"\n",
        "summary_multimodal(multimodal_model, eeg_input_size, spectrogram_input_size, CFG.device, output_image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-ZEdR0nEycg"
      },
      "outputs": [],
      "source": [
        "# Optimizer and Loss function\n",
        "optimizer = torch.optim.AdamW(multimodal_model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.KLDivLoss()\n",
        "# Optionally, define a learning rate scheduler\n",
        "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEjF5WWbEycg"
      },
      "outputs": [],
      "source": [
        "# Get the first batch from the train_loader\n",
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "# Extract the spectrogram, eeg, and label data from the first batch\n",
        "(eeg_data, spectrogram_data) , labels = first_batch\n",
        "\n",
        "# Extract the 5th element (sample) from the spectrogram data\n",
        "sample_spectrogram = spectrogram_data[10]\n",
        "\n",
        "# Print the shape to verify\n",
        "print(\"Spectrogram Data Shape:\", sample_spectrogram.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lXWa1KLEycg"
      },
      "outputs": [],
      "source": [
        "# Get the first batch from the train_loader\n",
        "first_batch = next(iter(train_loader))\n",
        "\n",
        "# Extract the spectrogram, eeg, and label data from the first batch\n",
        "(eeg_data, spectrogram_data) , labels = first_batch\n",
        "\n",
        "# Extract the 5th element (sample) from the spectrogram data\n",
        "sample_spectrogram = spectrogram_data[4]\n",
        "# Run training and validation\n",
        "train_losses, valid_losses, train_accuracies, valid_accuracies = train_and_validate_combined(multimodal_model, train_loader, valid_loader, CFG.EPOCHS, optimizer, criterion, CFG.device, CFG.checkpoint_dir, n=1, sample_spectrogram=sample_spectrogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlOxKJ4aEych"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(checkpoint_dir, checkpoint_filename, model, optimizer):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        train_losses = checkpoint['train_losses']\n",
        "        valid_losses = checkpoint['valid_losses']\n",
        "        train_accuracies = checkpoint['train_accuracies']\n",
        "        valid_accuracies = checkpoint['valid_accuracies']\n",
        "        lr_scheduler = checkpoint['lr_scheduler']\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}'\")\n",
        "        start_epoch = 0\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        train_accuracies = []\n",
        "        valid_accuracies = []\n",
        "        lr_scheduler = []\n",
        "\n",
        "    return start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies, lr_scheduler\n",
        "\n",
        "def load_checkpoint_data_full(checkpoint_dir, checkpoint_filename):\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n",
        "    if os.path.isfile(checkpoint_path):\n",
        "        print(f\"Loading checkpoint '{checkpoint_path}'\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        return checkpoint['train_losses'], checkpoint['valid_losses'], checkpoint['train_accuracies'], checkpoint['valid_accuracies'], checkpoint['lr_scheduler']\n",
        "    else:\n",
        "        print(f\"No checkpoint found at '{checkpoint_path}'\")\n",
        "        return [], [], [], [], []\n",
        "\n",
        "def plot_training_results(num_epochs, train_losses=None, valid_losses=None, train_accuracies=None, valid_accuracies=None, checkpoint_file=None):\n",
        "    \"\"\"\n",
        "    Plots the training and validation loss, accuracy, and learning rate over the given number of epochs.\n",
        "\n",
        "    Args:\n",
        "    - num_epochs (int): The number of epochs.\n",
        "    - train_losses (list): List of training losses.\n",
        "    - valid_losses (list): List of validation losses.\n",
        "    - train_accuracies (list): List of training accuracies.\n",
        "    - valid_accuracies (list): List of validation accuracies.\n",
        "    - checkpoint_file (str): Path to the checkpoint file to load data from.\n",
        "    \"\"\"\n",
        "    if checkpoint_file and os.path.isfile(checkpoint_file):\n",
        "        checkpoint = torch.load(checkpoint_file)\n",
        "        train_losses = checkpoint.get('train_losses', [])\n",
        "        valid_losses = checkpoint.get('valid_losses', [])\n",
        "        train_accuracies = checkpoint.get('train_accuracies', [])\n",
        "        valid_accuracies = checkpoint.get('valid_accuracies', [])\n",
        "        lr_scheduler = checkpoint.get('lr_scheduler', [])\n",
        "        num_epochs = CFG.EPOCHS\n",
        "\n",
        "    epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    # Plotting Loss\n",
        "    plt.subplot(1, 3, 1)\n",
        "    if train_losses and valid_losses:\n",
        "        plt.plot(epochs_range, train_losses, label='Train Loss')\n",
        "        plt.plot(epochs_range, valid_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    # Plotting Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if train_accuracies and valid_accuracies:\n",
        "        plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
        "        plt.plot(epochs_range, valid_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    # Plotting Learning Rate\n",
        "    plt.subplot(1, 3, 3)\n",
        "    if isinstance(lr_scheduler, list) and lr_scheduler:\n",
        "        plt.plot(epochs_range, lr_scheduler, label='Learning Rate')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    plt.title('Learning Rate Schedule')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_individual_metrics(checkpoint_dir, eeg_filename=\"eeg_checkpoint.pth.tar\", spec_filename=\"spec_checkpoint.pth.tar\", combined_filename=\"combined_checkpoint.pth.tar\"):\n",
        "    eeg_train_losses, eeg_valid_losses, eeg_train_accuracies, eeg_valid_accuracies,_ = load_checkpoint_data_full(checkpoint_dir, eeg_filename)\n",
        "    spec_train_losses, spec_valid_losses, spec_train_accuracies, spec_valid_accuracies,_ = load_checkpoint_data_full(checkpoint_dir, spec_filename)\n",
        "    combined_train_losses, combined_valid_losses, combined_train_accuracies, combined_valid_accuracies,_ = load_checkpoint_data_full(checkpoint_dir, combined_filename)\n",
        "\n",
        "    epochs_range = range(1, len(eeg_train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    # Plotting EEG Model\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(epochs_range, eeg_train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, eeg_valid_losses, label='Valid Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('EEG Model: Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(3, 2, 2)\n",
        "    plt.plot(epochs_range, eeg_train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, eeg_valid_accuracies, label='Valid Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('EEG Model: Training vs Validation Accuracy')\n",
        "\n",
        "    # Plotting Spectrogram Model\n",
        "    plt.subplot(3, 2, 3)\n",
        "    plt.plot(epochs_range, spec_train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, spec_valid_losses, label='Valid Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Spectrogram Model: Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.plot(epochs_range, spec_train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, spec_valid_accuracies, label='Valid Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Spectrogram Model: Training vs Validation Accuracy')\n",
        "\n",
        "    # Plotting Combined Model\n",
        "    plt.subplot(3, 2, 5)\n",
        "    plt.plot(epochs_range, combined_train_losses, label='Train Loss')\n",
        "    plt.plot(epochs_range, combined_valid_losses, label='Valid Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Combined Model: Training vs Validation Loss')\n",
        "\n",
        "    plt.subplot(3, 2, 6)\n",
        "    plt.plot(epochs_range, combined_train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs_range, combined_valid_accuracies, label='Valid Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.title('Combined Model: Training vs Validation Accuracy')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ8KRTh7Eych"
      },
      "outputs": [],
      "source": [
        "# Example usage for EEG model\n",
        "if CFG.EEG:\n",
        "    plot_training_results(\n",
        "        num_epochs=CFG.EPOCHS,\n",
        "        checkpoint_file=os.path.join(CFG.checkpoint_dir, \"eeg_checkpoint.pth.tar\")\n",
        "    )\n",
        "\n",
        "# Example usage for Spectrogram model\n",
        "if CFG.SPEC:\n",
        "    plot_training_results(\n",
        "        num_epochs=CFG.EPOCHS,\n",
        "        checkpoint_file=os.path.join(CFG.checkpoint_dir, \"spec_checkpoint.pth.tar\")\n",
        "    )\n",
        "\n",
        "# Example usage for Combined model\n",
        "if not CFG.EEG and not CFG.SPEC:\n",
        "    plot_training_results(\n",
        "        num_epochs=CFG.EPOCHS,\n",
        "        checkpoint_file=os.path.join(CFG.checkpoint_dir, \"combined_checkpoint.pth.tar\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TX4Z4KqEych"
      },
      "outputs": [],
      "source": [
        "# Plotting individual metrics for all models\n",
        "plot_individual_metrics(\n",
        "    checkpoint_dir=CFG.checkpoint_dir,\n",
        "    eeg_filename=\"eeg_checkpoint.pth.tar\",\n",
        "    spec_filename=\"spec_checkpoint.pth.tar\",\n",
        "    combined_filename=\"combined_checkpoint.pth.tar\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxBEWvgHEych"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from captum.attr import IntegratedGradients, visualization\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-hhrmBkEych"
      },
      "outputs": [],
      "source": [
        "checkpoint_filename = \"combined_checkpoint.pth.tar\"\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(multimodal_model.parameters(), lr=0.001)\n",
        "start_epoch, train_losses, valid_losses, train_accuracies, valid_accuracies = load_checkpoint(CFG.checkpoint_dir, checkpoint_filename, multimodal_model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAAbzgjTEych"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty list to store EEG data\n",
        "background_data = []\n",
        "\n",
        "# Define the number of samples you want to collect\n",
        "num_samples = 100  # Adjust this number based on your dataset size and diversity\n",
        "\n",
        "# Gather a smaller subset of EEG data for the background\n",
        "background_samples = []\n",
        "for i, ((eeg_data, _), _) in enumerate(train_loader):\n",
        "    if i >= 10:  # Collect data from the first 10 batches\n",
        "        break\n",
        "    background_samples.append(eeg_data)\n",
        "\n",
        "background_data = torch.cat(background_samples, dim=0)[:num_samples]\n",
        "print(background_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sguu7FXfEych"
      },
      "outputs": [],
      "source": [
        "# Assuming you select an example from the train loader\n",
        "for (eeg_data, _), _ in train_loader:\n",
        "    eeg_data_sample = eeg_data[:1]  # Use just one sample for SHAP analysis\n",
        "    break\n",
        "print(eeg_data_sample.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_Rzsmy8Eych"
      },
      "outputs": [],
      "source": [
        "# Move the background data and samples to analyze to the GPU\n",
        "background_data = background_data.to(CFG.device)\n",
        "eeg_data_sample = eeg_data_sample.to(CFG.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddjlOWArEyci"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "# Initialize the explainer with the model and a subset of the background data\n",
        "\n",
        "explainer = shap.GradientExplainer(multimodal_model.eeg_model, background_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7YLgQKmEyci"
      },
      "outputs": [],
      "source": [
        "# Explain the sample using the GradientExplainer\n",
        "shap_values = explainer.shap_values(eeg_data_sample)\n",
        "\n",
        "# Check the type and length of the shap_values\n",
        "print(f\"Type of SHAP values: {type(shap_values)}\")\n",
        "print(f\"Number of elements in SHAP values list: {len(shap_values)}\")\n",
        "\n",
        "# Print the shape of each element in the SHAP values list\n",
        "for i, shap_value in enumerate(shap_values):\n",
        "    print(f\"Shape of SHAP values for output {i}: {shap_value.shape}\")\n",
        "\n",
        "# Observe the SHAP values for the first output\n",
        "print(\"SHAP values for the first output:\")\n",
        "print(shap_values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnpGu4GIEyci"
      },
      "outputs": [],
      "source": [
        "# Combine EEG features and map features\n",
        "channel_names = CFG.eeg_features + [f\"{feat[0]}-{feat[1]}\" for feat in CFG.map_features if feat[0] != \"EKG\" and feat[1] != \"EKG\"]\n",
        "\n",
        "# Verify the total number of channels\n",
        "assert len(channel_names) == 37, f\"Expected 37 channels, but got {len(channel_names)}\"\n",
        "\n",
        "print(\"Total number of channels:\", len(channel_names))\n",
        "print(\"Channel names:\", channel_names)\n",
        "\n",
        "# Extract the number of classes and class names\n",
        "num_classes = CFG.n_classes\n",
        "class_names = CFG.classes\n",
        "\n",
        "# Print the number of classes and class names\n",
        "print(\"Number of classes:\", num_classes)\n",
        "print(\"Class names:\", class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlArFDeYEyci"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values[0].reshape(-1, 37), feature_names=channel_names, plot_type='dot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIXxYbckEyci"
      },
      "outputs": [],
      "source": [
        "def get_top_n_features(shap_values, class_index, n=5):\n",
        "    \"\"\"\n",
        "    Get the top n features with the highest absolute SHAP values for a specific class.\n",
        "\n",
        "    Parameters:\n",
        "    - shap_values: SHAP values array of shape (num_classes, 1, channels, samples).\n",
        "    - class_index: Index of the class to extract SHAP values for.\n",
        "    - n: Number of top features to extract.\n",
        "\n",
        "    Returns:\n",
        "    - indices: Indices of the top n features for the specified class.\n",
        "    \"\"\"\n",
        "    # Extract SHAP values for the specified class\n",
        "    class_shap_values = shap_values[class_index]\n",
        "\n",
        "    # Compute mean of absolute SHAP values across the samples (axis=-1) and the first two dimensions\n",
        "    mean_shap_values = np.mean(np.abs(class_shap_values), axis=(0, 1, 2)).flatten()\n",
        "\n",
        "    # Get indices of top n features\n",
        "    top_n_indices = np.argsort(mean_shap_values)[-n:][::-1]\n",
        "\n",
        "    return top_n_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fElxGOmyEyci"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "# Assuming shap_values is of shape (num_classes, 1, channels, samples)\n",
        "num_classes = 4  # Example value, replace with actual number of classes\n",
        "\n",
        "# Get top 5 channels for class 0\n",
        "top_5_channels_class_0 = get_top_n_features(shap_values, class_index=0, n=5)\n",
        "print(\"Top 5 channels for class 0:\", top_5_channels_class_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8-doy08Eyci"
      },
      "outputs": [],
      "source": [
        "def plot_mean_shap_values(shap_values, class_index, channel_names):\n",
        "    \"\"\"\n",
        "    Plot the mean absolute SHAP values for a specific class across all channels.\n",
        "\n",
        "    Parameters:\n",
        "    - shap_values: SHAP values array of shape (num_classes, 1, channels, samples).\n",
        "    - class_index: Index of the class to plot SHAP values for.\n",
        "    - channel_names: List of channel names corresponding to the channels dimension.\n",
        "    \"\"\"\n",
        "    # Convert shap_values to numpy array if it's not already\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = np.array(shap_values)\n",
        "\n",
        "    # Extract SHAP values for the specified class\n",
        "    class_shap_values = shap_values[class_index, 0, :, :]\n",
        "\n",
        "    # Compute mean of absolute SHAP values across the samples (axis=-1)\n",
        "    mean_shap_values = np.mean(np.abs(class_shap_values), axis=-1).flatten()\n",
        "\n",
        "    # Debugging: Check the shape and values of mean_shap_values\n",
        "    print(\"Shape of mean_shap_values:\", mean_shap_values.shape)\n",
        "    print(\"Mean SHAP values:\", mean_shap_values)\n",
        "\n",
        "    # Plot the mean SHAP values\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(mean_shap_values)), mean_shap_values, tick_label=channel_names)\n",
        "    plt.xlabel('Channels')\n",
        "    plt.ylabel('Mean Absolute SHAP Value')\n",
        "    plt.title(f'Mean Absolute SHAP Values for Class {class_index}')\n",
        "    plt.xticks(rotation=90)  # Rotate channel names for better readability\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTR2fCBwEyci"
      },
      "outputs": [],
      "source": [
        "def plot_mean_shap_values_scatter(shap_values, class_index, channel_names):\n",
        "    \"\"\"\n",
        "    Plot the mean absolute SHAP values for a specific class across all channels using a scatter plot.\n",
        "\n",
        "    Parameters:\n",
        "    - shap_values: SHAP values array of shape (num_classes, 1, channels, samples).\n",
        "    - class_index: Index of the class to plot SHAP values for.\n",
        "    - channel_names: List of channel names corresponding to the channels dimension.\n",
        "    \"\"\"\n",
        "    # Convert shap_values to numpy array if it's not already\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_values = np.array(shap_values)\n",
        "\n",
        "    # Extract SHAP values for the specified class\n",
        "    class_shap_values = shap_values[class_index, 0, :, :]\n",
        "\n",
        "    # Compute mean of absolute SHAP values across the samples (axis=-1)\n",
        "    mean_shap_values = np.mean(np.abs(class_shap_values), axis=-1).flatten()\n",
        "\n",
        "    # Debugging: Check the shape and values of mean_shap_values\n",
        "    print(\"Shape of mean_shap_values:\", mean_shap_values.shape)\n",
        "    print(\"Mean SHAP values:\", mean_shap_values)\n",
        "\n",
        "    # Normalize SHAP values for color scaling\n",
        "    norm_mean_shap_values = (mean_shap_values - mean_shap_values.min()) / (mean_shap_values.max() - mean_shap_values.min())\n",
        "\n",
        "    # Plot the mean SHAP values using a scatter plot with enhanced visibility\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    scatter = plt.scatter(range(len(mean_shap_values)), mean_shap_values,\n",
        "                          s=norm_mean_shap_values * 2000 + 50,  # Scaled for better visibility\n",
        "                          c=mean_shap_values, cmap='viridis', alpha=0.8, edgecolors='w', linewidth=0.5)\n",
        "    plt.xticks(ticks=range(len(channel_names)), labels=channel_names, rotation=90)\n",
        "    plt.xlabel('Channels')\n",
        "    plt.ylabel('Mean Absolute SHAP Value')\n",
        "    plt.title(f'Mean Absolute SHAP Values for Class {class_index}')\n",
        "    cbar = plt.colorbar(scatter, label='Mean Absolute SHAP Value')\n",
        "    cbar.set_label('Mean Absolute SHAP Value')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FfwRcGCEyci"
      },
      "outputs": [],
      "source": [
        "# Plot mean SHAP values for class 0\n",
        "plot_mean_shap_values(shap_values, class_index=0, channel_names=channel_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Hs1kik3Eyci"
      },
      "outputs": [],
      "source": [
        "# Plot mean SHAP values for class 0\n",
        "plot_mean_shap_values_scatter(shap_values, class_index=0, channel_names=channel_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNQpNZ9vEyci"
      },
      "outputs": [],
      "source": [
        "class RestructuredDataset(Dataset):\n",
        "    def __init__(self, combined_dataloader, shap_values, top_n_channels, class_index, threshold=0.5):\n",
        "        self.combined_dataloader = combined_dataloader\n",
        "        self.shap_values = shap_values\n",
        "        self.top_n_channels = top_n_channels\n",
        "        self.class_index = class_index\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # Get the top n channels based on SHAP values\n",
        "        self.top_channels = self.get_top_n_channels(shap_values, top_n_channels)\n",
        "        print(f\"Top channels indices: {self.top_channels}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.combined_dataloader.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (eeg_data, spectrogram_data), labels = self.combined_dataloader.dataset[idx]\n",
        "\n",
        "        # Restructure EEG data to select the top n channels\n",
        "        eeg_data = eeg_data.numpy()  # Convert tensor to numpy array\n",
        "        eeg_data = eeg_data.copy()   # Make a copy to avoid negative stride issues\n",
        "\n",
        "        # Validate top channels indices\n",
        "        max_index = eeg_data.shape[1]  # Number of channels\n",
        "        valid_indices = [i for i in self.top_channels if i < max_index]\n",
        "        if len(valid_indices) < self.top_n_channels:\n",
        "            print(f\"Warning: Only {len(valid_indices)} valid indices found out of {self.top_n_channels} requested.\")\n",
        "\n",
        "        eeg_data = eeg_data[:, valid_indices, :]\n",
        "        eeg_data = torch.tensor(eeg_data, dtype=torch.float32)  # Convert back to tensor\n",
        "\n",
        "        # Convert labels to binary\n",
        "        binary_labels = (labels[self.class_index] > 0.5).long()\n",
        "\n",
        "        return (eeg_data, spectrogram_data), binary_labels\n",
        "\n",
        "    def get_top_n_channels(self, shap_values, n):\n",
        "        \"\"\"\n",
        "        Get the top n features with the highest absolute SHAP values for a specific class.\n",
        "\n",
        "        Parameters:\n",
        "        - shap_values: SHAP values array of shape (num_classes, 1, channels, samples).\n",
        "        - n: Number of top features to extract.\n",
        "\n",
        "        Returns:\n",
        "        - indices: Indices of the top n features.\n",
        "        \"\"\"\n",
        "        # Convert shap_values to numpy array if it's not already\n",
        "        if isinstance(shap_values, list):\n",
        "            shap_values = np.array(shap_values)\n",
        "\n",
        "        # Extract SHAP values for the specified class\n",
        "        class_shap_values = shap_values[self.class_index, 0, :, :]\n",
        "\n",
        "        # Compute mean of absolute SHAP values across the samples (axis=-1)\n",
        "        mean_shap_values = np.mean(np.abs(class_shap_values), axis=-1).flatten()\n",
        "\n",
        "        # Debugging: Check the shape and values of mean_shap_values\n",
        "        print(\"Shape of mean_shap_values:\", mean_shap_values.shape)\n",
        "        print(\"Mean SHAP values:\", mean_shap_values)\n",
        "\n",
        "        # Get indices of top n features\n",
        "        top_n_indices = np.argsort(mean_shap_values)[-n:][::-1]\n",
        "        return top_n_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q2YEuuSEycj"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "class_index = 0  # specify the class index for binary classification\n",
        "threshold = 0.5  # specify the threshold for binary classification\n",
        "top_n_channels = 5  # specify the number of top channels to select\n",
        "\n",
        "restructured_dataset = RestructuredDataset(train_loader, shap_values, top_n_channels, class_index, threshold)\n",
        "restructured_dataloader = DataLoader(restructured_dataset, batch_size=CFG.batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vPsHzm7Eycj"
      },
      "outputs": [],
      "source": [
        "# Observe the shape of data in restructured_dataloader\n",
        "for i, ((eeg_data, spectrogram_data), labels) in enumerate(restructured_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"  EEG Data Shape: {eeg_data.shape}\")\n",
        "    print(f\"  Spectrogram Data Shape: {spectrogram_data.shape}\")\n",
        "    print(f\"  Labels Shape: {labels.shape}\")\n",
        "    if i == 0:  # Print shapes for only the first batch\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Vcd6m9LEycj"
      },
      "outputs": [],
      "source": [
        "class EEGOnlyDataset(Dataset):\n",
        "    def __init__(self, restructured_dataloader):\n",
        "        self.restructured_dataloader = restructured_dataloader\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.restructured_dataloader.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        (eeg_data, spectrogram_data), labels = self.restructured_dataloader.dataset[idx]\n",
        "        return eeg_data, labels\n",
        "\n",
        "# Create the EEGOnlyDataset\n",
        "eeg_only_dataset = EEGOnlyDataset(restructured_dataloader)\n",
        "\n",
        "# Create a DataLoader for the EEGOnlyDataset\n",
        "eeg_only_dataloader = DataLoader(eeg_only_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiyGFAJZEycj"
      },
      "outputs": [],
      "source": [
        "# Observe the shape of data in eeg_only_dataloader\n",
        "for i, (eeg_data, labels) in enumerate(eeg_only_dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"  EEG Data Shape: {eeg_data.shape}\")\n",
        "    print(f\"  Labels Shape: {labels.shape}\")\n",
        "    break  # Just print the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gw_XZMqEycj"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram_only(processed, labels, num_labels=10, save_path='processed_spectrogram.png'):\n",
        "    x_ticks = np.linspace(0, processed.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = [labels[i] for i in x_ticks]\n",
        "\n",
        "    plt.figure(figsize=(20, 8))  # Adjust the figure size as needed\n",
        "\n",
        "    plt.title(\"Processed Spectrogram\")\n",
        "    if processed.ndim == 3 and processed.shape[2] > 1:\n",
        "        plt.imshow(processed[:, :, 0], aspect='auto', cmap='viridis')\n",
        "    else:\n",
        "        plt.imshow(processed.squeeze(), aspect='auto', cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)  # Save the plot to a file\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBJ6JOE7Eycj"
      },
      "outputs": [],
      "source": [
        "def plot_processed_eeg_data(processed_data, cfg, title_processed='Processed EEG Data', max_channels=37):\n",
        "    \"\"\"\n",
        "    Plots processed EEG data including differential channels.\n",
        "\n",
        "    Parameters:\n",
        "    - processed_data: numpy array of shape (processed_channels, processed_samples) including differential channels\n",
        "    - cfg: configuration object with channel names and mappings\n",
        "    - title_processed: title for the processed data plot\n",
        "    - max_channels: maximum number of channels to plot to avoid oversized images\n",
        "    \"\"\"\n",
        "    processed_channels = cfg.in_channels  # Assuming this reflects the primary EEG channels without differentials\n",
        "    differential_channels = len(cfg.map_features)  # Number of differential channels\n",
        "\n",
        "    # Ensure the number of channels does not exceed max_channels\n",
        "    total_channels = min(processed_channels + differential_channels, max_channels)\n",
        "    fig, axes = plt.subplots(total_channels, 1, figsize=(15, 2 * total_channels), sharex='col')\n",
        "\n",
        "    # Combine eeg_features and map_features for y-labels\n",
        "    y_labels = cfg.eeg_features + [f'{a}-{b}' for a, b in cfg.map_features]\n",
        "\n",
        "    # Plot the primary processed channels\n",
        "    for i in range(min(processed_channels, total_channels)):\n",
        "        axes[i].plot(processed_data[i], color='b')\n",
        "        axes[i].set_ylabel(y_labels[i])\n",
        "        if i == 0:\n",
        "            axes[i].set_title(title_processed)\n",
        "\n",
        "    # Plot differential signals below the primary channels\n",
        "    differential_start = processed_channels  # Starting index for differential signals\n",
        "    for j in range(min(differential_channels, total_channels - processed_channels)):\n",
        "        k = differential_start + j\n",
        "        if k < total_channels:\n",
        "            axes[k].plot(processed_data[processed_channels + j], color='orange')\n",
        "            axes[k].set_ylabel(y_labels[processed_channels + j])\n",
        "\n",
        "    # Ensure labels and axes are correctly shown\n",
        "    for ax in axes:\n",
        "        ax.set_xlabel('Samples')\n",
        "        if not ax.lines:\n",
        "            ax.axis('off')  # Turn off empty plots\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4GrJ37pEycj"
      },
      "outputs": [],
      "source": [
        "data, label = train_dataset[10]  # Replace 0 with the index you want to visualize\n",
        "\n",
        "eeg_data, spectrogram_data = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmzt2B2bEycj"
      },
      "outputs": [],
      "source": [
        "# Plot processed EEG Data\n",
        "plot_processed_eeg_data(eeg_data.squeeze().cpu().numpy(), CFG, title_processed='Processed EEG Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLC7a09aEycj"
      },
      "outputs": [],
      "source": [
        "# Plot processed Spectrogram Data\n",
        "plot_spectrogram_only(spectrogram_data, labels=CFG.SPECTR_COLUMNS, num_labels=10, save_path='raw_spectrogram_sample.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPQWhieCEycj"
      },
      "outputs": [],
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn.functional import softmax\n",
        "# Generate Segments\n",
        "from skimage.segmentation import slic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhdEnMlhEycj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the spectrogram image\n",
        "image_path = 'raw_spectrogram_sample.png'\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "image = np.array(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FADSrJolEycj"
      },
      "outputs": [],
      "source": [
        "# Define the prediction function for LIME\n",
        "def predict_fn(images, model, device):\n",
        "    images = [transforms.ToTensor()(Image.fromarray(img)).unsqueeze(0).to(device) for img in images]\n",
        "    images = torch.cat(images, dim=0)\n",
        "    model.eval()  # Ensure the model is in evaluation mode\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "    probs = softmax(outputs, dim=1).cpu().numpy()\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbM4r7gqEycj"
      },
      "outputs": [],
      "source": [
        "CFG.get_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZNGYMJEEyck"
      },
      "outputs": [],
      "source": [
        "# Initialize the LIME image explainer\n",
        "explainer = lime_image.LimeImageExplainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNL8r88Eyck"
      },
      "outputs": [],
      "source": [
        "# Visualize Segments\n",
        "def plot_segments(image, segments):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(mark_boundaries(image / 255.0, segments))\n",
        "    plt.title(\"Segments\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSBqcwg4Eyck"
      },
      "outputs": [],
      "source": [
        "# Apply SLIC algorithm to segment the image\n",
        "segments = slic(image, n_segments=100, compactness=10, sigma=1)\n",
        "\n",
        "# Plot the segments to visualize them\n",
        "plot_segments(image, segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgCkev31Eyck"
      },
      "outputs": [],
      "source": [
        "# Explain the image using LIME\n",
        "explanation = explainer.explain_instance(image, lambda x: predict_fn(x, spectrogram_model, CFG.device), top_labels=1, hide_color=0, num_samples=100, segmentation_fn=lambda x: slic(x, n_segments=100, compactness=10, sigma=1))\n",
        "\n",
        "# Get the explanation for the top label\n",
        "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I5M6EUTEyck"
      },
      "outputs": [],
      "source": [
        "# Show the explanation\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(mark_boundaries(temp / 255.0, mask))\n",
        "plt.title('LIME Explanation for Spectrogram Model')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXzmC9FUEyck"
      },
      "outputs": [],
      "source": [
        "# Explain the image using LIME\n",
        "multimodal_explanation = explainer.explain_instance(image, lambda x: predict_fn(x, multimodal_model.spectrogram_model, CFG.device), top_labels=1, hide_color=0, num_samples=1000, segmentation_fn=lambda x: slic(x, n_segments=100, compactness=10, sigma=1))\n",
        "\n",
        "# Get the explanation for the top label\n",
        "multi_temp, multi_mask = multimodal_explanation.get_image_and_mask(multimodal_explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC2ANKAvEyck"
      },
      "outputs": [],
      "source": [
        "# Show the explanation\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(mark_boundaries(multi_temp / 255.0, multi_mask))\n",
        "plt.title('LIME Explanation for Multimodal Spectrogram Model')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V251XZyEyck"
      },
      "outputs": [],
      "source": [
        "def move_png_files(source_folder, destination_folder):\n",
        "    \"\"\"\n",
        "    Collects all PNG files from the source folder and moves them to the destination folder.\n",
        "\n",
        "    Parameters:\n",
        "    - source_folder (str): Path to the source folder containing the PNG files.\n",
        "    - destination_folder (str): Path to the destination folder where PNG files will be moved.\n",
        "    \"\"\"\n",
        "    # Ensure destination folder exists\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "\n",
        "    # Get list of all files in source folder\n",
        "    files = os.listdir(source_folder)\n",
        "\n",
        "    # Loop through files and move PNG files to destination folder\n",
        "    for file in files:\n",
        "        if file.lower().endswith('.png'):\n",
        "            src_path = os.path.join(source_folder, file)\n",
        "            dst_path = os.path.join(destination_folder, file)\n",
        "            shutil.move(src_path, dst_path)\n",
        "            print(f\"Moved: {file}\")\n",
        "\n",
        "# Usage example\n",
        "source_folder = '/home/data2/ronak/kou'\n",
        "destination_folder = '/home/data2/ronak/kou/image_outputs'\n",
        "move_png_files(source_folder, destination_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gHcHP1AEyck"
      },
      "outputs": [],
      "source": [
        "class CombinedDatasetSaliencyMap(Dataset):\n",
        "    def __init__(self, metadata, cfg=CFG, training_flag=False, augmentations=None, plot=False):\n",
        "        self.metadata = metadata\n",
        "        self.cfg = cfg\n",
        "        self.training_flag = training_flag\n",
        "        self.augmentations = augmentations\n",
        "        self.plot = plot\n",
        "\n",
        "        # Set the random seed for reproducibility\n",
        "        self.cfg.seed_everything(cfg)\n",
        "\n",
        "        # Feature to index mapping from CFG\n",
        "        self.feature_to_index = self.cfg.feature_to_index\n",
        "        self.differential_channels_start_index = len(self.feature_to_index)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.metadata.iloc[idx]\n",
        "\n",
        "        # Process EEG data\n",
        "        eeg_data = self.process_eeg(row)\n",
        "        eeg_label = self.get_label(row)\n",
        "\n",
        "        # Process Spectrogram data\n",
        "        spectrogram_data = self.process_spectrogram(row)\n",
        "        spectrogram_label = self.get_label(row)\n",
        "\n",
        "        # Ensure the labels are the same\n",
        "        assert torch.equal(eeg_label, spectrogram_label), \"Labels do not match!\"\n",
        "\n",
        "        # Set requires_grad=True for saliency map computation\n",
        "        eeg_data.requires_grad = True\n",
        "        spectrogram_data.requires_grad = True\n",
        "\n",
        "        return (eeg_data, spectrogram_data), eeg_label\n",
        "\n",
        "    def process_eeg(self, row):\n",
        "        eeg_id = row['eeg_id']\n",
        "        eeg = load_train_eeg_frame(eeg_id)\n",
        "        waves = eeg.values.T\n",
        "\n",
        "        if self.cfg.AUGMENT:\n",
        "            waves = self.mirror_eeg(waves)\n",
        "\n",
        "        waves = self.butter_bandpass_filter(waves, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate)\n",
        "        waves = self.handle_nan(waves)\n",
        "        waves = self.calculate_differential_signals(waves)\n",
        "        waves = self.denoise_filter(waves)\n",
        "        waves = self.normalize(waves)\n",
        "        waves = self.select_and_map_channels(waves, self.cfg.eeg_features, self.feature_to_index)\n",
        "        waves = self.pad_or_truncate(waves, self.cfg.fixed_length)\n",
        "        waves = waves[np.newaxis, ...]  # Add channel dimension for EEG data\n",
        "        return torch.tensor(waves, dtype=torch.float32)\n",
        "\n",
        "    def process_spectrogram(self, row):\n",
        "        spec_id = row['spectrogram_id']\n",
        "        raw_spectrogram = load_train_spectr_frame(spec_id)\n",
        "\n",
        "        if isinstance(raw_spectrogram, pd.DataFrame):\n",
        "            raw_spectrogram = raw_spectrogram.to_numpy()\n",
        "\n",
        "        offset = row.get(\"spectrogram_label_offset_seconds\", None)\n",
        "        if offset is not None:\n",
        "            offset = offset // 2\n",
        "            basic_spectrogram = raw_spectrogram[:, offset:offset + 300]\n",
        "            pad_size = max(0, 300 - basic_spectrogram.shape[1])\n",
        "            basic_spectrogram = np.pad(basic_spectrogram, ((0, 0), (0, pad_size)), mode='constant')\n",
        "        else:\n",
        "            basic_spectrogram = raw_spectrogram\n",
        "\n",
        "        spectrogram = basic_spectrogram.T\n",
        "        spectrogram = self.pad_or_truncate(spectrogram, self.cfg.image_size)\n",
        "        spectrogram = self.handle_nan(spectrogram)\n",
        "        spectrogram = self.baseline_correction(spectrogram)\n",
        "        spectrogram = self.apply_notch_filter(spectrogram)\n",
        "        spectrogram = self.smooth_spectrogram(spectrogram)\n",
        "        spectrogram = self.normalize_signal(spectrogram)\n",
        "        spectrogram = self.resample_spectrogram(spectrogram, self.cfg.image_size)\n",
        "        spectrogram = np.tile(spectrogram[..., None], (1, 1, 3))\n",
        "\n",
        "        if self.plot:\n",
        "            self.plot_spectrograms(basic_spectrogram, spectrogram, self.metadata.index.tolist(), num_labels=10)\n",
        "\n",
        "        if self.augmentations:\n",
        "            spectrogram = (spectrogram * 255).astype(np.uint8)\n",
        "            augmented = self.augmentations(image=spectrogram)\n",
        "            spectrogram = augmented['image']\n",
        "            spectrogram = spectrogram.float() / 255.0\n",
        "        else:\n",
        "            spectrogram = spectrogram.astype(np.float32)\n",
        "            spectrogram = torch.tensor(spectrogram).permute(2, 0, 1).float()\n",
        "\n",
        "        return spectrogram\n",
        "\n",
        "    def get_label(self, row):\n",
        "        label_name = row['expert_consensus']\n",
        "        label_idx = self.cfg.name2label[label_name]\n",
        "        label = labels_to_probabilities(label_idx, self.cfg.n_classes)\n",
        "        return label.clone().detach().float()\n",
        "\n",
        "    def handle_nan(self, data):\n",
        "        \"\"\"Handle NaN values by replacing them with the mean of the respective channels.\"\"\"\n",
        "        data = data[~np.isnan(data).all(axis=1)]\n",
        "        if data.size == 0:\n",
        "            data = np.zeros((self.cfg.in_channels + len(self.cfg.map_features), self.cfg.fixed_length))\n",
        "        else:\n",
        "            where_nan = np.isnan(data)\n",
        "            mean_values = np.nanmean(data, axis=1, keepdims=True)\n",
        "            mean_values[np.isnan(mean_values)] = 0\n",
        "            data[where_nan] = np.take(mean_values, np.where(where_nan)[0])\n",
        "        return data\n",
        "\n",
        "    def pad_or_truncate(self, data, length):\n",
        "        if isinstance(length, int):\n",
        "            if data.shape[1] < length:\n",
        "                padding = np.zeros((data.shape[0], length - data.shape[1]))\n",
        "                data = np.hstack((data, padding))\n",
        "            else:\n",
        "                data = data[:, :length]\n",
        "        elif isinstance(length, tuple):\n",
        "            target_rows, target_cols = length\n",
        "            if data.shape[0] < target_rows:\n",
        "                row_padding = np.zeros((target_rows - data.shape[0], data.shape[1]))\n",
        "                data = np.vstack((data, row_padding))\n",
        "            else:\n",
        "                data = data[:target_rows, :]\n",
        "            if data.shape[1] < target_cols:\n",
        "                col_padding = np.zeros((data.shape[0], target_cols - data.shape[1]))\n",
        "                data = np.hstack((data, col_padding))\n",
        "            else:\n",
        "                data = data[:, :target_cols]\n",
        "        return data\n",
        "\n",
        "    def butter_bandpass(self, lowcut, highcut, fs, order=5):\n",
        "        nyquist = 0.5 * fs\n",
        "        low = lowcut / nyquist\n",
        "        high = highcut / nyquist\n",
        "        return butter(order, [low, high], btype='band')\n",
        "\n",
        "    def butter_bandpass_filter(self, data, lowcut, highcut, fs, order=5):\n",
        "        b, a = self.butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        return lfilter(b, a, data, axis=1)\n",
        "\n",
        "    def calculate_differential_signals(self, data):\n",
        "        num_pairs = len(self.cfg.map_features)\n",
        "        differential_data = np.zeros((num_pairs, data.shape[1]))\n",
        "        for i, (feat_a, feat_b) in enumerate(self.cfg.map_features):\n",
        "            if feat_a in self.feature_to_index and feat_b in self.feature_to_index:\n",
        "                differential_data[i, :] = data[self.feature_to_index[feat_a], :] - data[self.feature_to_index[feat_b], :]\n",
        "            else:\n",
        "                print(f\"Feature {feat_a} or {feat_b} not found in feature_to_index\")\n",
        "        return np.vstack((data, differential_data))\n",
        "\n",
        "    def denoise_filter(self, x):\n",
        "        y = self.butter_bandpass_filter(x, self.cfg.bandpass_filter['low'], self.cfg.bandpass_filter['high'], self.cfg.sampling_rate, order=6)\n",
        "        y = (y + np.roll(y, -1) + np.roll(y, -2) + np.roll(y, -3)) / 4\n",
        "        y = y[:, 0:-1:4]\n",
        "        return y\n",
        "\n",
        "    def normalize(self, data):\n",
        "        mean = np.mean(data, axis=1, keepdims=True)\n",
        "        std = np.std(data, axis=1, keepdims=True)\n",
        "        return (data - mean) / (std + 1e-6)\n",
        "\n",
        "    def select_and_map_channels(self, data, channels, feature_to_index):\n",
        "        selected_indices = [feature_to_index[ch] for ch in channels if ch in feature_to_index]\n",
        "        differential_indices = list(range(self.differential_channels_start_index, self.differential_channels_start_index + len(self.cfg.map_features)))\n",
        "        selected_data = data[selected_indices + differential_indices, :]\n",
        "        return selected_data\n",
        "\n",
        "    def mirror_eeg(self, data):\n",
        "        indx1 = [self.cfg.feature_to_index[x] for x in self.cfg.LL + self.cfg.LP if x in self.cfg.feature_to_index]\n",
        "        indx2 = [self.cfg.feature_to_index[x] for x in self.cfg.RL + self.cfg.RP if x in self.cfg.feature_to_index]\n",
        "        data[indx1, :], data[indx2, :] = data[indx2, :], data[indx1, :]\n",
        "        return data\n",
        "\n",
        "    def baseline_correction(self, sig):\n",
        "        sig -= np.mean(sig, axis=0)\n",
        "        return sig\n",
        "\n",
        "    def normalize_signal(self, sig):\n",
        "        sig = np.nan_to_num(sig, nan=np.nanmean(sig))\n",
        "        return (sig - np.min(sig)) / (np.max(sig) - np.min(sig) + 1e-6)\n",
        "\n",
        "    def apply_notch_filter(self, sig, freq=60, fs=200, quality=30):\n",
        "        b, a = iirnotch(freq, quality, fs)\n",
        "        sig = filtfilt(b, a, sig, axis=0)\n",
        "        return sig\n",
        "\n",
        "    def smooth_spectrogram(self, sig, sigma=1.0):\n",
        "        sig = gaussian_filter(sig, sigma=sigma)\n",
        "        return sig\n",
        "\n",
        "    def resample_spectrogram(self, sig, target_shape):\n",
        "        sig = resize(sig, target_shape, mode='reflect', anti_aliasing=True)\n",
        "        return sig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-A1sd1FEyck"
      },
      "outputs": [],
      "source": [
        "# Assuming 'metadata' is a pandas DataFrame containing your dataset information\n",
        "single_metadata = metadata.iloc[[10]]  # Select a single row\n",
        "single_dataset = CombinedDatasetSaliencyMap(single_metadata, cfg=CFG, training_flag=False, augmentations=None, plot=False)\n",
        "single_dataloader = DataLoader(single_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohTS1_FfEyck"
      },
      "outputs": [],
      "source": [
        "single_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N88VUy9XEyck"
      },
      "outputs": [],
      "source": [
        "multimodal_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSrHhUgXEycl"
      },
      "outputs": [],
      "source": [
        "# Function to plot EEG saliency map\n",
        "def plot_eeg_saliency(saliency, cfg,max_channels=37):\n",
        "    processed_channels = cfg.in_channels  # Assuming this reflects the primary EEG channels without differentials\n",
        "    differential_channels = len(cfg.map_features)  # Number of differential channels\n",
        "\n",
        "    # Ensure the number of channels does not exceed max_channels\n",
        "    total_channels = min(processed_channels + differential_channels, max_channels)\n",
        "\n",
        "\n",
        "    # Combine eeg_features and map_features for y-labels\n",
        "    y_labels = cfg.eeg_features + [f'{a}-{b}' for a, b in cfg.map_features]\n",
        "    num_channels, num_samples = saliency.shape\n",
        "    fig, axes = plt.subplots(total_channels, 1, figsize=(15, 2 * total_channels), sharex=True)\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        axes[i].plot(saliency[i], color='r')\n",
        "        axes[i].set_ylabel(y_labels[i])\n",
        "\n",
        "    plt.xlabel('Samples')\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRRJ_L1WEycl"
      },
      "outputs": [],
      "source": [
        "# Function to plot Spectrogram saliency map\n",
        "def plot_spectrogram_saliency(saliency, labels, num_labels=10):\n",
        "    x_ticks = np.linspace(0, saliency.shape[1] - 1, num_labels).astype(int)\n",
        "    x_labels = [labels[i] for i in x_ticks]\n",
        "\n",
        "    plt.figure(figsize=(20, 8))\n",
        "    plt.title(\"Spectrogram Saliency Map\")\n",
        "    plt.imshow(saliency, aspect='auto', cmap='hot')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(ticks=x_ticks, labels=x_labels, rotation=90)\n",
        "    plt.gca().xaxis.set_tick_params(labelsize=10)\n",
        "    plt.gcf().subplots_adjust(bottom=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRNGbms0Eycl"
      },
      "outputs": [],
      "source": [
        "def generate_saliency_maps(model, dataloader):\n",
        "    model.eval()\n",
        "    for (eeg_data, spectrogram_data), label in dataloader:\n",
        "        eeg_data = eeg_data.to(CFG.device)\n",
        "        spectrogram_data = spectrogram_data.to(CFG.device)\n",
        "        label = label.to(CFG.device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(eeg_data, spectrogram_data)\n",
        "        output_idx = output.argmax(dim=1)\n",
        "        output_max = output[0, output_idx]\n",
        "\n",
        "        # Retain gradients for non-leaf tensors\n",
        "        eeg_data.retain_grad()\n",
        "        spectrogram_data.retain_grad()\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Backward pass for EEG saliency\n",
        "        output_max.backward(retain_graph=True)\n",
        "        eeg_saliency = eeg_data.grad.data.abs().squeeze().cpu().numpy()\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Backward pass for Spectrogram saliency\n",
        "        output_max.backward()\n",
        "        spectrogram_saliency = spectrogram_data.grad.data.abs().max(dim=1, keepdim=True)[0].squeeze().cpu().numpy()\n",
        "\n",
        "        # Plot the saliency maps\n",
        "        plot_eeg_saliency(eeg_saliency, CFG)\n",
        "        plot_spectrogram_saliency(spectrogram_saliency, CFG.SPECTR_COLUMNS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBMaWsACEycl"
      },
      "outputs": [],
      "source": [
        "data, label = single_dataset[0]  # Replace 0 with the index you want to visualize\n",
        "\n",
        "eeg_data, spectrogram_data = data\n",
        "spectrogram_data = spectrogram_data.detach().numpy()\n",
        "\n",
        "\n",
        "\n",
        "plot_spectrogram_only(spectrogram_data, labels=CFG.SPECTR_COLUMNS, num_labels=10, save_path='raw_spectrogram_sample.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liq2UKUwEycl"
      },
      "outputs": [],
      "source": [
        "# Generate saliency maps\n",
        "generate_saliency_maps(multimodal_model, single_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCRkUX72Eycl"
      },
      "outputs": [],
      "source": [
        "print(\"ronak\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99FrrgcVLoSa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
